[
["index.html", "A Guide to Crawl-ing with R About the Authors Preface", " A Guide to Crawl-ing with R a book based on the ‘learning to crawl’ workshop(s) Josh M. London and Devin S. Johnson 2017-02-23 About the Authors Drs. Josh M. London and Devin S. Johnson are researchers at the NOAA Alaska Fisheries Science Center’s Marine Mammal Laboratory in Seattle, Washington. Dr. London has over 10 years of experience programming and deploying satellite tags on phocid seals. He has also developed various workflows for the management of telemetry data in R. Dr. Johnson is a leading biometrician with expertise in the analysis of animal movement. Dr. Johnson is the lead author and developer of the R package crawl. Preface This book is being developed simultaneously with a 3-day workshop on the use of the crawl package for analysis of animal movment in R. Significant components of this book, example code, and content will change. Please use code and examples with caution and contact the authors before relying on advice, code, or examples for real-world analysis. Identification of errors is encouraged and folks should open an issue via the GitHub repository. We will also accept pull requests for any bug fixes or enhancements. A significant portion of this book is devoted to the support of the R package crawl. The package is available on CRAN as well as GitHub. If you are using crawl as part of a publication, please refer to the output from citation('crawl') for an appropriate citation. One should also consider referencing the original Ecology paper from 2008. Johnson, D. S., London, J. M., Lea, M.-A. and Durban, J. W. (2008), CONTINUOUS-TIME CORRELATED RANDOM WALK MODEL FOR ANIMAL TELEMETRY DATA. Ecology, 89: 1208–1215. doi:10.1890/07-1032.1 Disclaimer This book is a scientific product and is not official communication of the Alaska Fisheries Science Center, the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All AFSC Marine Mammal Laboratory (AFSC-MML) GitHub project code is provided on an ‘as is’ basis and the user assumes responsibility for its use. AFSC-MML has relinquished control of the information and no longer has responsibility to protect the integrity, confidentiality, or availability of the information. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this GitHub project will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government. "],
["crawl-theory.html", "1 The Mathematics and Statistics of Animal Movement 1.1 Mathematics 1.2 Statistics", " 1 The Mathematics and Statistics of Animal Movement 1.1 Mathematics 1.1.1 Discrete-time random walks 1.1.2 Correlated random walks 1.1.3 Brownian motion 1.1.4 Ornstein-Ulenbeck (OU) process 1.1.5 Stochastic differential equations 1.1.6 Integrated SDEs 1.1.7 Continuous-time CRW 1.2 Statistics 1.2.1 Maximum likelihood inference 1.2.2 Bayesian inference 1.2.3 State-space models 1.2.4 Kalman filter/smoother (KFS) 1.2.5 Practical Bayesian inference 1.2.6 Process imputation "],
["crawl-practical.html", "2 A Pragmatic Guide for Analysis with crawl 2.1 Analysis and Coding Principles 2.2 Assembling Source Data 2.3 Tidy Data for Telemetry 2.4 Preparing Input Data for crawl 2.5 Determining Your Model Parameters 2.6 Exploring and Troubleshooting Model Results 2.7 Predicting a Movement Track 2.8 Simulating Tracks from the Posterior 2.9 Visualization of Results", " 2 A Pragmatic Guide for Analysis with crawl The crawl package is designed and built with the idea that it should be accessible and useful to a research biologist with some intermediate R skills and an understanding of the basic statistical theory behind the analysis of animal movement. This portion of the book will focus on providing the user with suggested principles, workflows, and pragmatic approaches that, if followed, should make analysis with crawl more efficient and reliable. This content is broken up in the following sections: Analysis and Coding Principles Assembling Source Data Tidy Data for Telemetry Preparing Input Data for crawl Determining Your Model Parameters Exploring and Troubleshooting Model Results Predicting a Movement Track Simulating Tracks from the Posterior Visualization of Results 2.1 Analysis and Coding Principles As with anything in science and R, there are a number of right ways to approach a problem. The workflows and principles outline here aren’t the only way to use crawl. However, this content has been developed after years of working with researchers and troubleshooting common issues. For most users, following this guide will prove a successful endeavor. More advanced users or those with specific needs should feel free to refer to this as a starting point but then expand to meet their need. 2.1.1 Source Data are Read Only Source data should not be edited. In many cases, the financial and personal investment required to acquire these data is significant. In addition, the unique timing, location, and nature of the data cannot be replicated so every effort should be employed to preserve the integrity of source data as they were collected. All efforts should be made to also insure that source data are stored in plain text or well described open formats. This approach provides researchers confidence that data will be available and usable well into the future. 2.1.2 Script Everything In all likelihood, the format of any source data will not be condusive to established analytical procedures. For example, crawl cannot accept a raw data file downloaded from ArgosWeb or the Wildlife Computers Data Portal. Some amount of data carpentry is required. Keeping with the principles of reproducibility, all data assembly and carpentry should be scripted. Here, we rely on the R programming language for our scripts, but Python or other similar languages will also meet this need. 2.1.3 Document Along the Way Scripting the data assembly should be combined with an effort to properly document the process. Documentation is an important component of reproducibility and, if done as part of the scritping and development process, provides an easier workflow for publishing results and data to journals and repositories. The rmarkdown, bookdown, and knitr packages provide an excellent framework for combining your R scripts with documentation. This entire book is written with bookdown and knitr. 2.1.4 Embrace the Tidyverse The tidyverse describes a set of R packages developed by, or in association with, Hadley Wickham. Tidy data principles are outlined in a 2014 paper published in the Journal of Statistical Software. The key tennants of tidy data are: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Location data from telemetry deployments often follows this type of structure: each location estimate is a row and the columns all represent variable attributes associated with that location. Additionally, the location data are usually organized into a single table. Behavior data, however, often comes in a less structured format. Different behavior type data are sometimes mixed within a single table and column headings do not consistently describe the same variable (e.g. Bin1, Bin2, etc). There are valid reasons for tag vendors and data providers to transmit the source data in this way, but the structure is not condusive to analysis. The tidyverse package is a wrapper for a number of separate packages that each contribute to the tidy’ing of data. The tidyr, dplyr, readr, and purrr packages are key components. In addition, the lubridate package (not included in the tidyverse package) is especially useful for consistent maninpulation of date-time values. 2.1.5 Anticipate Errors &amp; Trap Them Use R’s error trapping functions (try() and tryCatch()) in areas of your code where you anticipate parsing errors or potential issues with convergence or fit. The purrr::safely() function also provides a great service. 2.2 Assembling Source Data Where and how you access the source data for your telemetry study will depend on the type of tag and vendor. Argos location data is available to users from the Argos website and many third party sites and respositories (e.g. movebank, sea-turtle.org) have API connections to ArgosWeb that can faciliate data access. Each tag manufacturer often has additional data streams included within the Argos transmission that require specific software or processing to translate. Both the Sea Mammal Research Unit (SMRU) and Wildlife Computers provide online data portals that provide users access to the location and additional sensor data. Regardless of how the data are retrieved, these files should be treated as read only and not edited. ArgosWeb and vendors have, typically, kept the data formats and structure consistent over time. This affords end users the ability to develop custom processing scripts without much fear the formats will change and break their scripts. 2.2.1 Read In CSV Source Data Here we will demonstrate the development of such a script using data downloaded from the Wildlife Computers data portal. All data are grouped by deployment and presented in various comma-separated files. The *-Locations.csv contains all of the Argos location estimates determined for the deployment. At a minimum, this file includes identifying columns such as DeployID, PTT, Date, Quality, Type, Latitude, and Longitude. If the Argos Kalman filtering approach has been enabled (which it should be for any modern tag deployment), then additional data will be found in columns that describe the error ellipses (e.g. Error Semi-major axis, Error Semi-minor axis, Error Ellipse orientation). Note, if the tag transmitted GPS/FastLoc data, this file will list those records as Type = 'FastGPS'. The other file of interest we will be working with is an example *-Histos.csv. These files include dive and haul-out behavior data derived from sensors on the tag. Most notably, the pressure transducer (for depth) and the salt-water switch (for determining wet/dry status). We will focus on processing this file to demonstrate how we can properly tidy our data. We will rely on the tidyverse set of packages plus purrr and lubridate to make reading and processing these files easier and more reliable. library(tidyverse) library(purrr) library(lubridate) The readr includes the read_csv() function which we will rely on to read the csv data into R. path_to_file &lt;- &quot;examples/data/160941-Locations.csv&quot; tbl_locs &lt;- readr::read_csv(path_to_file) 2.2.2 Specify Column Data Types The readr::read_csv() function tries to interpret the proper data types for each of the columns and provides us the column specifications it determined. In most cases, the function gets this correct. However, if we examine the cols() specification selected, we see that the Date column was read in as a character data type. To correct this, we need to provide our own cols() specification. We can do this by simply modifying the specification readr::read_csv() provided us. In this case, we want the Date column to rely on col_datetime and to parse the character string using the format %H:%M:%S %d-%b-%Y. my_cols &lt;- cols( DeployID = col_character(), Ptt = col_integer(), Instr = col_character(), Date = col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), Type = col_character(), Quality = col_character(), Latitude = col_double(), Longitude = col_double(), `Error radius` = col_integer(), `Error Semi-major axis` = col_integer(), `Error Semi-minor axis` = col_integer(), `Error Ellipse orientation` = col_integer(), Offset = col_character(), `Offset orientation` = col_character(), `GPE MSD` = col_character(), `GPE U` = col_character(), Count = col_character(), Comment = col_character() ) tbl_locs &lt;- readr::read_csv(path_to_file,col_types = my_cols) 2.2.3 Import From Many Files In most instances you’ll likely want to read in data from multiple deployments. The purrr::map() function can be combined with readr::read_csv() and dplyr::bind_rows() make this easy. tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() tbl_locs %&gt;% group_by(DeployID) %&gt;% summarise(num_locs = n(), start_date = min(Date), end_date = max(Date)) ## # A tibble: 2 × 4 ## DeployID num_locs start_date end_date ## &lt;chr&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 PV2016_3019_16A0222 3064 2016-09-21 02:54:02 2017-02-15 08:15:22 ## 2 PV2016_3024_15A0909 976 2016-09-22 02:48:00 2017-02-15 05:06:20 2.3 Tidy Data for Telemetry A key tennat of tidy data is that each row represents a single observation. For location data, regardless of the data source, this is typical of the source data structure. Each line in a file usually represents a single location estimate at a specific time. So, there’s very little we need to do in order to get our tbl_locs into an acceptable structure. 2.3.1 Standard Column Names One thing we can do, is to adjust the column names so there are no spaces or hyphens. We can also drop everything to lower-case to avoid typos. To do this we will create our own function, make_names(), which expands on the base function make.names(). We also want to change the date column to a more appropriate and less confusing date_time. make_names &lt;- function(x) { new_names &lt;- make.names(colnames(x)) new_names &lt;- gsub(&quot;\\\\.&quot;, &quot;_&quot;, new_names) new_names &lt;- tolower(new_names) colnames(x) &lt;- new_names x } tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::rename(date_time = date) %&gt;% dplyr::arrange(deployid, date_time) At this point, our tbl_locs is pretty tidy and, other than a few final steps, ready for input into the crawl::crwMLE() function. The *-Histos.csv file, however, has a messy, horizontal structure and will require some tidying. The tidyr package has a number of options that will help us. But, first let’s look closer at the structure of a typical *-Histos.csv. 2.3.2 Tidy a Messy ‘Histos’ File Wildlife Computers provides a variety of data types within this file structure. Because the details of each data type are subject to tag programming details, the data structure places observations into a series of Bin1, Bin2, ..., Bin72 columns. What each Bin# column represents depends on the data type and how the tags were programmed. For instance, a Dive Depth record represents the number of dives with a maximum depth within the programmed depth bin range. Time At Depth (TAD) represents the number of seconds spent within a particular programmed depth bin range. Each record represents a period of time that is also user programmable. While this messy data structure is an efficient way of delivering the data to users, it violates the principles of tidy data strucutes and presents a number of data management and analysis challenges. We are going to focus on a process for tidying the Percent or 1Percent timeline data as this information can be incorporated into the crawl::crwMLE() model fit. The same approach should be adopted for other histogram record types and the wcUtils package provides a series of helper functions developed specifically for this challenge. The Percent/1Percent timeline data are presented within the *-Histos.csv file structure as 24-hour observations — each record represents a 24-hour UTC time period. Within the record, each Bin# represents an hour of the day from 00 through 23. The values recorded for each Bin# column represent the percentage of that hour the tag was dry (out of the water). This data structure violates the tidy principles of requiring each record represent a single observation and that each column represent a single variable. Let’s start by reading the data in to R with readr::read_csv() as before. Note, we again rely on a custom cols() specification to insure data types are properly set. And, since we are only interested in Bin columns 1:24, we will select out the remaining columns. my_cols &lt;- readr::cols( DeployID = readr::col_character(), Ptt = readr::col_character(), DepthSensor = readr::col_character(), Source = readr::col_character(), Instr = readr::col_character(), HistType = readr::col_character(), Date = readr::col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), `Time Offset` = readr::col_double(), Count = readr::col_integer(), BadTherm = readr::col_integer(), LocationQuality = readr::col_character(), Latitude = readr::col_double(), Longitude = readr::col_double(), NumBins = readr::col_integer(), Sum = readr::col_integer(), Bin1 = readr::col_double(), Bin2 = readr::col_double(), Bin3 = readr::col_double(), Bin4 = readr::col_double(), Bin5 = readr::col_double(), Bin6 = readr::col_double(), Bin7 = readr::col_double(), Bin8 = readr::col_double(), Bin9 = readr::col_double(), Bin10 = readr::col_double(), Bin11 = readr::col_double(), Bin12 = readr::col_double(), Bin13 = readr::col_double(), Bin14 = readr::col_double(), Bin15 = readr::col_double(), Bin16 = readr::col_double(), Bin17 = readr::col_double(), Bin18 = readr::col_double(), Bin19 = readr::col_double(), Bin20 = readr::col_double(), Bin21 = readr::col_double(), Bin22 = readr::col_double(), Bin23 = readr::col_double(), Bin24 = readr::col_double(), Bin25 = readr::col_double(), Bin26 = readr::col_double(), Bin27 = readr::col_double(), Bin28 = readr::col_double(), Bin29 = readr::col_double(), Bin30 = readr::col_double(), Bin31 = readr::col_double(), Bin32 = readr::col_double(), Bin33 = readr::col_double(), Bin34 = readr::col_double(), Bin35 = readr::col_double(), Bin36 = readr::col_double(), Bin37 = readr::col_double(), Bin38 = readr::col_double(), Bin39 = readr::col_double(), Bin40 = readr::col_double(), Bin41 = readr::col_double(), Bin42 = readr::col_double(), Bin43 = readr::col_double(), Bin44 = readr::col_double(), Bin45 = readr::col_double(), Bin46 = readr::col_double(), Bin47 = readr::col_double(), Bin48 = readr::col_double(), Bin49 = readr::col_double(), Bin50 = readr::col_double(), Bin51 = readr::col_double(), Bin52 = readr::col_double(), Bin53 = readr::col_double(), Bin54 = readr::col_double(), Bin55 = readr::col_double(), Bin56 = readr::col_double(), Bin57 = readr::col_double(), Bin58 = readr::col_double(), Bin59 = readr::col_double(), Bin60 = readr::col_double(), Bin61 = readr::col_double(), Bin62 = readr::col_double(), Bin63 = readr::col_double(), Bin64 = readr::col_double(), Bin65 = readr::col_double(), Bin66 = readr::col_double(), Bin67 = readr::col_double(), Bin68 = readr::col_double(), Bin69 = readr::col_double(), Bin70 = readr::col_double(), Bin71 = readr::col_double(), Bin72 = readr::col_double() ) tbl_percent &lt;- dir(&#39;examples/data/&#39;, &quot;*-Histos.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::select(-(bin25:bin72)) %&gt;% dplyr::arrange(deployid, date) 2.3.3 Gathering Data with tidyr Now that our percent timeline data are in R, we can work to tidy the data structure using the tidyr::gather() function. This function requires that we provide character names for the key and value columns to create. We then specify which columns will be gathered into those key-value paired columns. In this case, our new columns will be bin and percent_dry and we will gather the values from all of the columns that start with the character string bin. Since we know that each bin column reprents an hour of the day, we can also improve our data structure by changing the value of date_hour (note we rename date to date_hour) to reflect this. We accomplish this by creating a simple lookup table (tibble) that relates each bin column to the hour of day it represents. The dplyr::left_join() function is used to match and merge this with our tbl_percent and then we add the hour value to each date_hour value with the dplyr::mutate() and lubridate::hours() functions. ## Create a tbl_df that Relates Bin Columns to Day Hours bins &lt;- tibble(bin = paste(&quot;bin&quot;,1:24,sep = &quot;&quot;),hour = 0:23) ## Chain Together Multiple Commands to Create Our Tidy Dataset tbl_percent &lt;- tbl_percent %&gt;% tidyr::gather(bin,percent_dry, starts_with(&#39;bin&#39;)) %&gt;% dplyr::left_join(bins, by = &quot;bin&quot;) %&gt;% dplyr::rename(date_hour = date) %&gt;% dplyr::mutate(date_hour = date_hour + lubridate::hours(hour)) %&gt;% dplyr::select(deployid,date_hour,percent_dry) %&gt;% dplyr::arrange(deployid,date_hour) Examining tbl_percent, we see that the layout is now more vertical and more in line with tidy principles tbl_percent ## # A tibble: 5,568 × 3 ## deployid date_hour percent_dry ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 PV2016_3019_16A0222 2016-09-21 02:00:00 NA ## 2 PV2016_3019_16A0222 2016-09-21 03:00:00 NA ## 3 PV2016_3019_16A0222 2016-09-21 04:00:00 100 ## 4 PV2016_3019_16A0222 2016-09-21 05:00:00 100 ## 5 PV2016_3019_16A0222 2016-09-21 06:00:00 100 ## 6 PV2016_3019_16A0222 2016-09-21 07:00:00 30 ## 7 PV2016_3019_16A0222 2016-09-21 08:00:00 20 ## 8 PV2016_3019_16A0222 2016-09-21 09:00:00 10 ## 9 PV2016_3019_16A0222 2016-09-21 10:00:00 10 ## 10 PV2016_3019_16A0222 2016-09-21 11:00:00 10 ## # ... with 5,558 more rows 2.3.4 Visualizing Source Data To further explore our imported data and confirm there aren’t any remaining issues to address, we can create visual representations of the data. The first plot will rely on the leaflet package to create an interactive plot of the tbl_locs object. The second plot will create a heatmap of tbl_percent values. library(leaflet) library(leaflet.extras) library(sf) sf_locs &lt;- st_as_sf(tbl_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% st_set_crs(&quot;+init=epsg:4326&quot;) %&gt;% dplyr::mutate(id = ifelse(deployid == unique(deployid)[1], 1, 2)) sf_lines &lt;- sf_locs %&gt;% st_geometry() %&gt;% st_cast(&quot;MULTIPOINT&quot;,ids = sf_locs$id) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) %&gt;% st_sf(deployid = as.factor(unique(sf_locs$deployid))) pal &lt;- colorFactor(ggthemes::ptol_pal()(2), domain = sf_lines$deployid) m &lt;- leaflet(data = sf_locs) %&gt;% addProviderTiles(&quot;Esri.OceanBasemap&quot;) %&gt;% addPolylines(data = sf_lines, weight = 2, color = ~pal(deployid)) %&gt;% addCircleMarkers(radius = 2, weight = 2, opacity = 1, color = ~pal(deployid)) %&gt;% addLegend(pal = pal,values = ~deployid,labels = ~deployid) %&gt;% addMiniMap(tiles = providers$Esri.OceanBasemap, toggleDisplay = T) %&gt;% suspendScroll() m Figure 2.1: An interactive map of observed satellite telemetry locations from two harbor seals in the Aleutian Islands. library(viridis) library(ggthemes) p &lt;- tbl_percent %&gt;% dplyr::mutate(solar_hour = date_hour + lubridate::dhours(11.5), solar_hour = lubridate::hour(solar_hour)) %&gt;% ggplot(aes(x = lubridate::date(date_hour), y = solar_hour, fill = percent_dry)) + geom_tile() + scale_y_continuous(breaks = c(6,12,18)) + scale_fill_viridis(name = &quot;Percent Dry&quot;, guide = guide_colorbar( title.position = &quot;bottom&quot;, barwidth = 25, barheight = 1 )) + facet_wrap( ~ deployid, nrow = 2) + theme_fivethirtyeight() + theme(axis.title = element_text()) + ylab(&quot;local solar hour&quot;) + xlab(&quot;&quot;) + ggtitle(paste(&quot;Haul-out Behavior by Local Solar Hour&quot;)) + theme(panel.grid.major.x = element_blank()) p Figure 2.2: Depiction of hourly percent dry observations for two satellite tags deployed on harbor seals in the Aleutian Islands. 2.4 Preparing Input Data for crawl At this point, the tbl_locs object represents the minimal source data required to fit crawl::crwMLE(). In addition to including location estimates in the model fit, one can also include an activity parameter. For harbor seals, we can rely on the percent-timeline data as an indicator of activity — when the tags are dry for most of an hour, the seal is likely hauled out and not moving. The crawl::crwMLE() will interpret this as an indication that the auto-correlation parameter should go to zero. In this section, we will prepare two input objects for crawl::crwMLE(). The first will be a spatial object of location estimates that can be passed directly. The second, will be a data.frame that represents a merge between the location estimates and percent-timeline data. 2.4.1 Error Parameters for GPS Data Many tags deployed on marine animals now include the option to collect GPS quality location esimates in addition to Argos. crawl::crwMLE() relies on the ellipse error parameters provide by Argos, so we need to fill in the GPS locations with appropriate values. GPS locations from these tags are generally expected to have an error radius of ~50m as long as the number of satellites used for the estimate is greater than 4. Below, we will use the dplyr::mutate() function to fill in these values. While we are at it, we will also remove any Argos records that do not have associated error ellipse values. tbl_locs &lt;- tbl_locs %&gt;% dplyr::filter(!(is.na(error_radius) &amp; type == &#39;Argos&#39;)) %&gt;% dplyr::mutate( error_semi_major_axis = ifelse(type == &#39;FastGPS&#39;, 50, error_semi_major_axis), error_semi_minor_axis = ifelse(type == &#39;FastGPS&#39;, 50, error_semi_minor_axis), error_ellipse_orientation = ifelse(type == &#39;FastGPS&#39;, 0, error_ellipse_orientation) ) 2.4.2 Known Locations In many cases, researchers are also knowledgable of the time and location coordinates of animal release. The Wildlife Computers Data Portal allows this to be entered and is included as a User record. The error radius one might specify for this record is dependent on the release situation. In general, it is best to over estimate this error radius. This record should be the first location record in the dataset passed to crawl::crwMLE(). tbl_locs &lt;- tbl_locs %&gt;% dplyr::mutate( error_semi_major_axis = ifelse(type == &#39;User&#39;, 500, error_semi_major_axis), error_semi_minor_axis = ifelse(type == &#39;User&#39;, 500, error_semi_minor_axis), error_ellipse_orientation = ifelse(type == &#39;User&#39;, 0, error_ellipse_orientation) ) user_dt &lt;- tbl_locs %&gt;% dplyr::rename(release_dt = date_time) %&gt;% dplyr::filter(type == &#39;User&#39;) %&gt;% select(deployid,release_dt) tbl_locs &lt;- tbl_locs %&gt;% left_join(user_dt, by = &#39;deployid&#39;) %&gt;% filter(date_time &gt;= release_dt) %&gt;% select(-release_dt) tbl_locs %&gt;% group_by(deployid) %&gt;% do(head(.,1)) ## Source: local data frame [2 x 18] ## Groups: deployid [2] ## ## deployid ptt instr date_time type quality ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 PV2016_3019_16A0222 164831 Mk10 2016-09-21 05:08:00 User &lt;NA&gt; ## 2 PV2016_3024_15A0909 160941 Mk10 2016-09-22 02:48:00 User &lt;NA&gt; ## # ... with 12 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, ## # error_radius &lt;int&gt;, error_semi_major_axis &lt;dbl&gt;, ## # error_semi_minor_axis &lt;dbl&gt;, error_ellipse_orientation &lt;dbl&gt;, ## # offset &lt;chr&gt;, offset_orientation &lt;chr&gt;, gpe_msd &lt;chr&gt;, gpe_u &lt;chr&gt;, ## # count &lt;chr&gt;, comment &lt;chr&gt; 2.4.3 Course Speed Filter This step is optional, but it is very common for Argos data to include obviously wrong locations (locations that are 100s of kilometers away from the study area). Including such obviously wrong locations in the analysis can result in unexpected issues or problems fitting. For this reason, we recommend a course speed filter to remove these obviously wrong locations. A typical speed filter might use a value of 2.5 m/s as a biologically reasonable value for pinnipeds. For this application, we will use 7.5 m/s and rely on the argosfilter package. The example code below follows a typical split-apply-combine approach using functions from the dplyr, tidyr, and purrr packages. To better understand what each step is doing, we suggest you step through the process and examine the results at point. speed_filt &lt;- function(x) { argosfilter::sdafilter( lat = x$latitude, lon = x$longitude, dtime = x$date_time, lc = x$quality, vmax = 7.5 ) } tbl_locs &lt;- tbl_locs %&gt;% dplyr::group_by(deployid) %&gt;% tidyr::nest() %&gt;% dplyr::mutate(filtered = purrr::map(data, speed_filt)) %&gt;% tidyr::unnest() %&gt;% dplyr::filter(filtered %in% c(&quot;not&quot;, &quot;end location&quot;)) %&gt;% dplyr::select(-filtered) %&gt;% dplyr::arrange(deployid,date_time) 2.4.4 Duplicate Times Argos data often contain near duplicate records. These are identified by location estimates with the same date-time but differing coordinate or error values. In theory, crawl::crwMLE() can handle these situations, but we have found it is more reliable to fix these records. The first option for fixing the records would be to eliminate one of the duplicate records. However, it is often not possible to reliably identify which record is more appropriate to discard. For this reason, we advocate adjusting the date-time value for one of the records and increasing the value by 1 second. To facilitate this, we will rely on the xts::make.time.unique() function. make_unique &lt;- function(x) { xts::make.time.unique(x$date_time,eps = 1) } library(xts) tbl_locs &lt;- tbl_locs %&gt;% dplyr::arrange(deployid,date_time) %&gt;% dplyr::group_by(deployid) %&gt;% tidyr::nest() %&gt;% dplyr::mutate(unique_time = purrr::map(data, make_unique)) %&gt;% tidyr::unnest() %&gt;% dplyr::select(-date_time) %&gt;% rename(date_time = unique_time) 2.4.5 Create a Spatial Object There are now two package frameworks within R for describing spatial data: sp and sf. The sp package has been the defacto standard for spatial data in R and has widespread support in a number of additional packages. The sf package is new and still in active development. However, the sf package is based on the more open simple features standard for spatial data. For this reason, and others, many in the R community feel sf is the future of spatial data in R. The crawl::crwMLE() can accept either a SpatialPointsDataFrame from the sp package or an sf data.frame of POINT geometry types. Here, we will focus on the sf package. sf_locs &lt;- sf::st_as_sf(tbl_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% sf::st_set_crs(.,4326) Earlier versions of crawl allowed users to pass geographic coordinates (e.g. latitude, longitude). However, relying on geographic coordinates presents some problems (e.g. crossing 180 or 0) and requires some assumptions to convert decimal degrees into meters — the units for the provided error ellipses. Because of these issues, all data should be provided to the crawl::crwMLE() as projected coordinates in meter units. Which projection users choose is dependent upon their study area. For these data, we will go with the North Pole LAEA Bering Sea projection. This projection is often abbreciated by the EPSG code of 3571. The sf::transform() function provides an easy method for transforming our coordinates. After, we can examine the geometry and confirm our coordinate units are in meters. sf_locs &lt;- sf::st_transform(sf_locs, 3571) sf::st_geometry(sf_locs) ## Geometry set for 3779 features ## geometry type: POINT ## dimension: XY ## bbox: xmin: -510531.2 ymin: -4094081 xmax: -369854 ymax: -4034591 ## epsg (SRID): 3571 ## proj4string: +proj=laea +lat_0=90 +lon_0=180 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## First 5 geometries: At this point, sf_locs is a valid format for input to crawl::crwMLE(). However, if you want to include the percent-timeline data as an activity parameter, there’s bit more work to do. 2.4.6 Merging with Activity Data Since our activity data represent hourly records, we need to create a date_hour field within tbl_locs as the basis for our join. sf_locs &lt;- sf_locs %&gt;% mutate(date_hour = lubridate::floor_date(date_time,&#39;hour&#39;)) %&gt;% arrange(deployid, date_time) We also need to make sure our activity data includes all possible hours during the deployment. There are often missing days where the percent-timeline data was not transmitted back and so are not included in the source data. This will results in NA values for percent_dry. We will set these values to 33 because it will allow some level of correlated movement during these missing periods. Another option would be to use the zoo::na_locf() function to carry the last ovserved value forward. tbl_percent &lt;- tbl_percent %&gt;% group_by(deployid) %&gt;% tidyr::complete(date_hour = full_seq(date_hour, 3600L)) %&gt;% dplyr::mutate(percent_dry = ifelse(is.na(percent_dry), 33, percent_dry)) In addition to joining these two tables together, we also need to make sure the activity data doesn’t extend beyond the range of date-time values for observed locations. trim_deployment &lt;- function(x) { dplyr::filter(x, between(date_hour, lubridate::floor_date(min(date_time,na.rm = TRUE),&#39;hour&#39;), max(date_time,na.rm = TRUE))) } sf_locs_activity &lt;- tbl_percent %&gt;% dplyr::left_join(sf_locs, by = c(&quot;deployid&quot;,&quot;date_hour&quot;)) %&gt;% dplyr::group_by(deployid) %&gt;% tidyr::nest() %&gt;% dplyr::mutate(data = purrr::map(data,trim_deployment)) %&gt;% unnest() %&gt;% dplyr::mutate(date_time = ifelse(is.na(date_time),date_hour,date_time), date_time = lubridate::as_datetime(date_time)) %&gt;% dplyr::arrange(deployid,date_time) The last step we need to complete is to translate the simple feature geometry for our point coordinates into an x and y column. coords &lt;- sf_locs_activity$geometry %&gt;% map(unlist) coords_df &lt;- list() for (i in 1:length(coords)) { coords_df[[i]] &lt;- data.frame(x = ifelse(!is.null(coords[[i]]), coords[[i]][,1], NA), y = ifelse(!is.null(coords[[i]]), coords[[i]][,2], NA)) } sf_locs_activity &lt;- sf_locs_activity %&gt;% bind_cols(bind_rows(coords_df)) 2.5 Determining Your Model Parameters 2.6 Exploring and Troubleshooting Model Results 2.7 Predicting a Movement Track 2.8 Simulating Tracks from the Posterior 2.9 Visualization of Results "]
]
