[
["index.html", "A Guide to Crawl-ing with R About the Authors Preface", " A Guide to Crawl-ing with R a book based on the ‘learning to crawl’ workshop(s) Josh M. London and Devin S. Johnson 2017-02-21 About the Authors Drs. Josh M. London and Devin S. Johnson are researchers at the NOAA Alaska Fisheries Science Center’s Marine Mammal Laboratory in Seattle, Washington. Dr. London has over 10 years of experience programming and deploying satellite tags on phocid seals. He has also developed various workflows for the management of telemetry data in R. Dr. Johnson is a leading biometrician with expertise in the analysis of animal movement. Dr. Johnson is the lead author and developer of the R package crawl. Preface This book is being developed simultaneously with a 3-day workshop on the use of the crawl package for analysis of animal movment in R. Significant components of this book, example code, and content will change. Please use code and examples with caution and contact the authors before relying on advice, code, or examples for real-world analysis. Identification of errors is encouraged and folks should open an issue via the GitHub repository. We will also accept pull requests for any bug fixes or enhancements. A significant portion of this book is devoted to the support of the R package crawl. The package is available on CRAN as well as GitHub. If you are using crawl as part of a publication, please refer to the output from citation('crawl') for an appropriate citation. One should also consider referencing the original Ecology paper from 2008. Johnson, D. S., London, J. M., Lea, M.-A. and Durban, J. W. (2008), CONTINUOUS-TIME CORRELATED RANDOM WALK MODEL FOR ANIMAL TELEMETRY DATA. Ecology, 89: 1208–1215. doi:10.1890/07-1032.1 Disclaimer This book is a scientific product and is not official communication of the Alaska Fisheries Science Center, the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All AFSC Marine Mammal Laboratory (AFSC-MML) GitHub project code is provided on an ‘as is’ basis and the user assumes responsibility for its use. AFSC-MML has relinquished control of the information and no longer has responsibility to protect the integrity, confidentiality, or availability of the information. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this GitHub project will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government. "],
["crawl-theory.html", "1 The Mathematics and Statistics of Animal Movement 1.1 Mathematics 1.2 Statistics", " 1 The Mathematics and Statistics of Animal Movement 1.1 Mathematics 1.1.1 Discrete-time random walks 1.1.2 Correlated random walks 1.1.3 Brownian motion 1.1.4 Ornstein-Ulenbeck (OU) process 1.1.5 Stochastic differential equations 1.1.6 Integrated SDEs 1.1.7 Continuous-time CRW 1.2 Statistics 1.2.1 Maximum likelihood inference 1.2.2 Bayesian inference 1.2.3 State-space models 1.2.4 Kalman filter/smoother (KFS) 1.2.5 Practical Bayesian inference 1.2.6 Process imputation "],
["crawl-practical.html", "2 A Pragmatic Guide for Analysis with crawl 2.1 Analysis and Coding Principles 2.2 Assembling Source Data 2.3 Tidy Data for Telemetry 2.4 Preparing Input Data for crawl 2.5 Determining Your Model Parameters 2.6 Exploring and Troubleshooting Model Results 2.7 Predicting a Movement Track 2.8 Simulating Tracks from the Posterior 2.9 Visualization of Results", " 2 A Pragmatic Guide for Analysis with crawl The crawl package is designed and built with the idea that it should be accessible and useful to a research biologist with some intermediate R skills and an understanding of the basic statistical theory behind the analysis of animal movement. This portion of the book will focus on providing the user with suggested principles, workflows, and pragmatic approaches that, if followed, should make analysis with crawl more efficient and reliable. This content is broken up in the following sections: Analysis and Coding Principles Assembling Source Data Tidy Data for Telemetry Preparing Input Data for crawl Determining Your Model Parameters Exploring and Troubleshooting Model Results Predicting a Movement Track Simulating Tracks from the Posterior Visualization of Results 2.1 Analysis and Coding Principles As with anything in science and R, there are a number of right ways to approach a problem. The workflows and principles outline here aren’t the only way to use crawl. However, this content has been developed after years of working with researchers and troubleshooting common issues. For most users, following this guide will prove a successful endeavor. More advanced users or those with specific needs should feel free to refer to this as a starting point but then expand to meet their need. 2.1.1 Source Data are Read Only Source data should not be edited. In many cases, the financial and personal investment required to acquire these data is significant. In addition, the unique timing, location, and nature of the data cannot be replicated so every effort should be employed to preserve the integrity of source data as they were collected. All efforts should be made to also insure that source data are stored in plain text or well described open formats. This approach provides researchers confidence that data will be available and usable well into the future. 2.1.2 Script Everything In all likelihood, the format of any source data will not be condusive to established analytical procedures. For example, crawl cannot accept a raw data file downloaded from ArgosWeb or the Wildlife Computers Data Portal. Some amount of data carpentry is required. Keeping with the principles of reproducibility, all data assembly and carpentry should be scripted. Here, we rely on the R programming language for our scripts, but Python or other similar languages will also meet this need. 2.1.3 Document Along the Way Scripting the data assembly should be combined with an effort to properly document the process. Documentation is an important component of reproducibility and, if done as part of the scritping and development process, provides an easier workflow for publishing results and data to journals and repositories. The rmarkdown, bookdown, and knitr packages provide an excellent framework for combining your R scripts with documentation. This entire book is written with bookdown and knitr. 2.1.4 Embrace the Tidyverse The tidyverse describes a set of R packages developed by, or in association with, Hadley Wickham. Tidy data principles are outlined in a 2014 paper published in the Journal of Statistical Software. The key tennants of tidy data are: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Location data from telemetry deployments often follows this type of structure: each location estimate is a row and the columns all represent variable attributes associated with that location. Additionally, the location data are usually organized into a single table. Behavior data, however, often comes in a less structured format. Different behavior type data are sometimes mixed within a single table and column headings do not consistently describe the same variable (e.g. Bin1, Bin2, etc). There are valid reasons for tag vendors and data providers to transmit the source data in this way, but the structure is not condusive to analysis. The tidyverse package is a wrapper for a number of separate packages that each contribute to the tidy’ing of data. The tidyr, dplyr, readr, and purrr packages are key components. In addition, the lubridate package (not included in the tidyverse package) is especially useful for consistent maninpulation of date-time values. 2.1.5 Anticipate Errors &amp; Trap Them Use R’s error trapping functions (try() and tryCatch()) in areas of your code where you anticipate parsing errors or potential issues with convergence or fit. The purrr::safely() function also provides a great service. 2.2 Assembling Source Data Where and how you access the source data for your telemetry study will depend on the type of tag and vendor. Argos location data is available to users from the Argos website and many third party sites and respositories (e.g. movebank, sea-turtle.org) have API connections to ArgosWeb that can faciliate data access. Each tag manufacturer often has additional data streams included within the Argos transmission that require specific software or processing to translate. Both the Sea Mammal Research Unit (SMRU) and Wildlife Computers provide online data portals that provide users access to the location and additional sensor data. Regardless of how the data are retrieved, these files should be treated as read only and not edited. ArgosWeb and vendors have, typically, kept the data formats and structure consistent over time. This affords end users the ability to develop custom processing scripts without much fear the formats will change and break their scripts. Here we will demonstrate the development of such a script using data downloaded from the Wildlife Computers data portal. All data are grouped by deployment and presented in various comma-separated files. The *-Locations.csv contains all of the Argos location estimates determined for the deployment. At a minimum, this file includes identifying columns such as DeployID, PTT, Date, Quality, Type, Latitude, and Longitude. If the Argos Kalman filtering approach has been enabled (which it should be for any modern tag deployment), then additional data will be found in columns that describe the error ellipses (e.g. Error Semi-major axis, Error Semi-minor axis, Error Ellipse orientation). Note, if the tag transmitted GPS/FastLoc data, this file will list those records as Type = 'FastGPS'. The other file of interest we will be working with is an example *-Histos.csv. These files include dive and haul-out behavior data derived from sensors on the tag. Most notably, the pressure transducer (for depth) and the salt-water switch (for determining wet/dry status). We will focus on processing this file to demonstrate how we can properly tidy our data. We will rely on the tidyverse set of packages plus purrr and lubridate to make reading and processing these files easier and more reliable. library(tidyverse) library(purrr) library(lubridate) The readr includes the read_csv() function which we will rely on to read the csv data into R. path_to_file &lt;- &quot;examples/data/160941-Locations.csv&quot; tbl_locs &lt;- readr::read_csv(path_to_file) The readr::read_csv() function tries to interpret the proper data types for each of the columns and provides us the column specifications it determined. In most cases, the function gets this correct. However, if we examine the cols() specification selected, we see that the Date column was read in as a character data type. To correct this, we need to provide our own cols() specification. We can do this by simply modifying the specification readr::read_csv() provided us. In this case, we want the Date column to rely on col_datetime and to parse the character string using the format %H:%M:%S %d-%b-%Y. my_cols &lt;- cols( DeployID = col_character(), Ptt = col_integer(), Instr = col_character(), Date = col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), # changed from col_character() Type = col_character(), Quality = col_character(), Latitude = col_double(), Longitude = col_double(), `Error radius` = col_integer(), `Error Semi-major axis` = col_integer(), `Error Semi-minor axis` = col_integer(), `Error Ellipse orientation` = col_integer(), Offset = col_character(), `Offset orientation` = col_character(), `GPE MSD` = col_character(), `GPE U` = col_character(), Count = col_character(), Comment = col_character() ) tbl_locs &lt;- readr::read_csv(path_to_file,col_types = my_cols) In most instances you’ll likely want to read in data from multiple deployments. The purrr::map() function can be combined with readr::read_csv() and dplyr::bind_rows() make this easy. tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() 2.3 Tidy Data for Telemetry A key tennat of tidy data is that each row represents a single observation. For location data, regardless of the data source, this is typical of the source data structure. Each line in a file usually represents a single location estimate at a specific time. So, there’s very little we need to do in order to get our tbl_locs into an acceptable structure. One thing we can do, is to adjust the column names so there are no spaces or hyphens. We can also drop everything to lower-case to avoid typos. To do this we will create our own function, make_names(), which expands on the base function make.names(). We also want to change the date column to a more appropriate and less confusing date_time. make_names &lt;- function(x) { new_names &lt;- make.names(colnames(x)) new_names &lt;- gsub(&quot;\\\\.&quot;, &quot;_&quot;, new_names) new_names &lt;- tolower(new_names) colnames(x) &lt;- new_names x } tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::rename(date_time = date) %&gt;% dplyr::arrange(deployid, date_time) At this point, our tbl_locs is pretty tidy and, other than a few final steps, ready for input into the crawl::crwMLE() function. The *-Histos.csv file, however, has a messy, horizontal structure and will require some tidying. The tidyr package has a number of options that will help us. But, first let’s look closer at the structure of a typical *-Histos.csv. Wildlife Computers provides a variety of data types within this file structure. Because the details of each data type are subject to tag programming details, the data structure places observations into a series of Bin1, Bin2, ..., Bin72 columns. What each Bin# column represents depends on the data type and how the tags were programmed. For instance, a Dive Depth record represents the number of dives with a maximum depth within the programmed depth bin range. Time At Depth (TAD) represents the number of seconds spent within a particular programmed depth bin range. Each record represents a period of time that is also user programmable. While this messy data structure is an efficient way of delivering the data to users, it violates the principles of tidy data strucutes and presents a number of data management and analysis challenges. We are going to focus on a process for tidying the Percent or 1Percent timeline data as this information can be incorporated into the crawl::crwMLE() model fit. The same approach should be adopted for other histogram record types and the wcUtils package provides a series of helper functions developed specifically for this challenge. The Percent/1Percent timeline data are presented within the *-Histos.csv file structure as 24-hour observations — each record represents a 24-hour UTC time period. Within the record, each Bin# represents an hour of the day from 00 through 23. The values recorded for each Bin# column represent the percentage of that hour the tag was dry (out of the water). This data structure violates the tidy principles of requiring each record represent a single observation and that each column represent a single variable. Let’s start by reading the data in to R with readr::read_csv() as before. Note, we again rely on a custom cols() specification to insure data types are properly set. And, since we are only interested in Bin columns 1:24, we will select out the remaining columns. my_cols &lt;- readr::cols( DeployID = readr::col_character(), Ptt = readr::col_character(), DepthSensor = readr::col_character(), Source = readr::col_character(), Instr = readr::col_character(), HistType = readr::col_character(), Date = readr::col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), `Time Offset` = readr::col_double(), Count = readr::col_integer(), BadTherm = readr::col_integer(), LocationQuality = readr::col_character(), Latitude = readr::col_double(), Longitude = readr::col_double(), NumBins = readr::col_integer(), Sum = readr::col_integer(), Bin1 = readr::col_double(), Bin2 = readr::col_double(), Bin3 = readr::col_double(), Bin4 = readr::col_double(), Bin5 = readr::col_double(), Bin6 = readr::col_double(), Bin7 = readr::col_double(), Bin8 = readr::col_double(), Bin9 = readr::col_double(), Bin10 = readr::col_double(), Bin11 = readr::col_double(), Bin12 = readr::col_double(), Bin13 = readr::col_double(), Bin14 = readr::col_double(), Bin15 = readr::col_double(), Bin16 = readr::col_double(), Bin17 = readr::col_double(), Bin18 = readr::col_double(), Bin19 = readr::col_double(), Bin20 = readr::col_double(), Bin21 = readr::col_double(), Bin22 = readr::col_double(), Bin23 = readr::col_double(), Bin24 = readr::col_double(), Bin25 = readr::col_double(), Bin26 = readr::col_double(), Bin27 = readr::col_double(), Bin28 = readr::col_double(), Bin29 = readr::col_double(), Bin30 = readr::col_double(), Bin31 = readr::col_double(), Bin32 = readr::col_double(), Bin33 = readr::col_double(), Bin34 = readr::col_double(), Bin35 = readr::col_double(), Bin36 = readr::col_double(), Bin37 = readr::col_double(), Bin38 = readr::col_double(), Bin39 = readr::col_double(), Bin40 = readr::col_double(), Bin41 = readr::col_double(), Bin42 = readr::col_double(), Bin43 = readr::col_double(), Bin44 = readr::col_double(), Bin45 = readr::col_double(), Bin46 = readr::col_double(), Bin47 = readr::col_double(), Bin48 = readr::col_double(), Bin49 = readr::col_double(), Bin50 = readr::col_double(), Bin51 = readr::col_double(), Bin52 = readr::col_double(), Bin53 = readr::col_double(), Bin54 = readr::col_double(), Bin55 = readr::col_double(), Bin56 = readr::col_double(), Bin57 = readr::col_double(), Bin58 = readr::col_double(), Bin59 = readr::col_double(), Bin60 = readr::col_double(), Bin61 = readr::col_double(), Bin62 = readr::col_double(), Bin63 = readr::col_double(), Bin64 = readr::col_double(), Bin65 = readr::col_double(), Bin66 = readr::col_double(), Bin67 = readr::col_double(), Bin68 = readr::col_double(), Bin69 = readr::col_double(), Bin70 = readr::col_double(), Bin71 = readr::col_double(), Bin72 = readr::col_double() ) tbl_percent &lt;- dir(&#39;examples/data/&#39;, &quot;*-Histos.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::select(-(bin25:bin72)) %&gt;% dplyr::arrange(deployid, date) Now that our percent timeline data are in R, we can work to tidy the data structure using the tidyr::gather() function. This function requires that we provide character names for the key and value columns to create. We then specify which columns will be gathered into those key-value paired columns. In this case, our new columns will be bin and percent_dry and we will gather the values from all of the columns that start with the character string bin. Since we know that each bin column reprents an hour of the day, we can also improve our data structure by changing the value of date_hour (note we rename date to date_hour) to reflect this. We accomplish this by creating a simple lookup table (tibble) that relates each bin column to the hour of day it represents. The dplyr::left_join() function is used to match and merge this with our tbl_percent and then we add the hour value to each date_hour value with the dplyr::mutate() and lubridate::hours() functions. ## Create a tbl_df that Relates Bin Columns to Day Hours bins &lt;- tibble(bin=paste(&quot;bin&quot;,1:24,sep=&quot;&quot;),hour=0:23) ## Chain Together Multiple Commands to Create Our Tidy Dataset tbl_percent &lt;- tbl_percent %&gt;% tidyr::gather(bin,percent_dry, starts_with(&#39;bin&#39;)) %&gt;% dplyr::left_join(bins, by=&quot;bin&quot;) %&gt;% dplyr::rename(date_hour = date) %&gt;% dplyr::mutate(date_hour = date_hour + lubridate::hours(hour)) %&gt;% dplyr::select(deployid,date_hour,percent_dry) %&gt;% dplyr::arrange(deployid,date_hour) Examining tbl_percent, we see that the layout is now more vertical and more in line with tidy principles tbl_percent ## # A tibble: 5,568 × 3 ## deployid date_hour percent_dry ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 PV2016_3019_16A0222 2016-09-21 02:00:00 NA ## 2 PV2016_3019_16A0222 2016-09-21 03:00:00 NA ## 3 PV2016_3019_16A0222 2016-09-21 04:00:00 100 ## 4 PV2016_3019_16A0222 2016-09-21 05:00:00 100 ## 5 PV2016_3019_16A0222 2016-09-21 06:00:00 100 ## 6 PV2016_3019_16A0222 2016-09-21 07:00:00 30 ## 7 PV2016_3019_16A0222 2016-09-21 08:00:00 20 ## 8 PV2016_3019_16A0222 2016-09-21 09:00:00 10 ## 9 PV2016_3019_16A0222 2016-09-21 10:00:00 10 ## 10 PV2016_3019_16A0222 2016-09-21 11:00:00 10 ## # ... with 5,558 more rows To further explore our imported data and confirm there aren’t any remaining issues to address, we can create visual representations of the data. The first plot will rely on the leaflet package to create an interactive plot of the tbl_locs object. The second plot will create a heatmap of tbl_percent values. library(leaflet) library(leaflet.extras) library(sf) sf_locs &lt;- st_as_sf(tbl_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% st_set_crs(&quot;+init=epsg:4326&quot;) %&gt;% dplyr::mutate(id = ifelse(deployid == unique(deployid)[1], 1, 2)) sf_lines &lt;- sf_locs %&gt;% st_geometry() %&gt;% st_cast(&quot;MULTIPOINT&quot;,ids = sf_locs$id) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) %&gt;% st_sf(deployid = as.factor(unique(sf_locs$deployid))) pal &lt;- colorFactor(ggthemes::ptol_pal()(2), domain = sf_lines$deployid) m &lt;- leaflet(data = sf_locs) %&gt;% addProviderTiles(&quot;Esri.OceanBasemap&quot;) %&gt;% addPolylines(data = sf_lines, weight = 2, color = ~pal(deployid)) %&gt;% addCircleMarkers(radius = 2, weight = 2, opacity = 1, color = ~pal(deployid)) %&gt;% addLegend(pal = pal,values = ~deployid,labels = ~deployid) %&gt;% addMiniMap(tiles = providers$Esri.OceanBasemap, toggleDisplay = T) %&gt;% suspendScroll() m Figure 2.1: An interactive map of observed satellite telemetry locations from two harbor seals in the Aleutian Islands. library(viridis) library(ggthemes) p &lt;- tbl_percent %&gt;% dplyr::mutate(solar_hour = date_hour + lubridate::dhours(11.5), solar_hour = lubridate::hour(solar_hour)) %&gt;% ggplot(aes(x = lubridate::date(date_hour), y = solar_hour, fill = percent_dry)) + geom_tile() + scale_y_continuous(breaks = c(6,12,18)) + scale_fill_viridis(name = &quot;Percent Dry&quot;, guide = guide_colorbar( title.position = &quot;bottom&quot;, barwidth = 25, barheight = 1 )) + facet_wrap( ~ deployid, nrow = 2) + theme_fivethirtyeight() + theme(axis.title = element_text()) + ylab(&quot;local solar hour&quot;) + xlab(&quot;&quot;) + ggtitle(paste(&quot;Haul-out Behavior by Local Solar Hour&quot;)) + theme(panel.grid.major.x = element_blank()) p Figure 2.2: Depiction of hourly percent dry observations for two satellite tags deployed on harbor seals in the Aleutian Islands. 2.4 Preparing Input Data for crawl 2.5 Determining Your Model Parameters 2.6 Exploring and Troubleshooting Model Results 2.7 Predicting a Movement Track 2.8 Simulating Tracks from the Posterior 2.9 Visualization of Results "]
]
