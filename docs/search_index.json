[
["index.html", "A Guide to Crawl-ing with R About the Authors Preface", " A Guide to Crawl-ing with R a book based on the ‘learning to crawl’ workshop(s) Josh M. London and Devin S. Johnson 2017-03-01 About the Authors Drs. Josh M. London and Devin S. Johnson are researchers at the NOAA Alaska Fisheries Science Center’s Marine Mammal Laboratory in Seattle, Washington. Dr. London has over 10 years of experience programming and deploying satellite tags on phocid seals. He has also developed various workflows for the management of telemetry data in R. Dr. Johnson is a leading biometrician with expertise in the analysis of animal movement. Dr. Johnson is the lead author and developer of the R package crawl. Preface This book is being developed simultaneously with a 3-day workshop on the use of the crawl package for analysis of animal movment in R. Significant components of this book, example code, and content will change. Please use code and examples with caution and contact the authors before relying on advice, code, or examples for real-world analysis. Identification of errors is encouraged and folks should open an issue via the GitHub repository. We will also accept pull requests for any bug fixes or enhancements. A significant portion of this book is devoted to the support of the R package crawl. The package is available on CRAN as well as GitHub. Many of the functions and code examples provided within this book are only available within the ‘devel’ version of crawl from GitHub. if (!require(devtools)) install.packages(&#39;devtools&#39;) devtools::install_github(&#39;NMML/crawl&#39;,ref = &#39;devel&#39;) If you are using crawl as part of a publication, please refer to the output from citation('crawl') for an appropriate citation. One should also consider referencing the original Ecology paper from 2008. Johnson, D. S., London, J. M., Lea, M.-A. and Durban, J. W. (2008), CONTINUOUS-TIME CORRELATED RANDOM WALK MODEL FOR ANIMAL TELEMETRY DATA. Ecology, 89: 1208–1215. doi:10.1890/07-1032.1 Disclaimer This book is a scientific product and is not official communication of the Alaska Fisheries Science Center, the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All AFSC Marine Mammal Laboratory (AFSC-MML) GitHub project code is provided on an ‘as is’ basis and the user assumes responsibility for its use. AFSC-MML has relinquished control of the information and no longer has responsibility to protect the integrity, confidentiality, or availability of the information. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this GitHub project will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government. "],
["crawl-theory.html", "1 The Mathematics and Statistics of Animal Movement 1.1 Mathematics 1.2 Statistics", " 1 The Mathematics and Statistics of Animal Movement 1.1 Mathematics 1.1.1 Discrete-time random walks 1.1.2 Correlated random walks 1.1.3 Brownian motion 1.1.4 Ornstein-Ulenbeck (OU) process 1.1.5 Stochastic differential equations 1.1.6 Integrated SDEs 1.1.7 Continuous-time CRW 1.2 Statistics 1.2.1 Maximum likelihood inference 1.2.2 Bayesian inference 1.2.3 State-space models 1.2.4 Kalman filter/smoother (KFS) 1.2.5 Practical Bayesian inference 1.2.6 Process imputation "],
["crawl-practical.html", "2 A Pragmatic Guide for Analysis with crawl 2.1 Analysis and Coding Principles 2.2 Assembling Source Data 2.3 Tidy Data for Telemetry 2.4 Preparing Input Data for crawl 2.5 Determining Your Model Parameters 2.6 Fitting with crawl::crwMLE() 2.7 Exploring and Troubleshooting Model Results 2.8 Predicting a Movement Track 2.9 Adjust Path Around Land 2.10 Simulating Tracks from the Posterior 2.11 Visualization of Results", " 2 A Pragmatic Guide for Analysis with crawl The crawl package is designed and built with the idea that it should be accessible and useful to a research biologist with some intermediate R skills and an understanding of the basic statistical theory behind the analysis of animal movement. This portion of the book will focus on providing the user with suggested principles, workflows, and pragmatic approaches that, if followed, should make analysis with crawl more efficient and reliable. Telemetry data are collected on a wide range of species and come in a number of formats and data structures. The code and examples provided here were developed from data the authors are most familiar with. You will very likely NOT be able to just copy and paste the code and apply to your data. We suggest you focus more on understanding what the code is doing and then write your own version of the code to meet your data and your research needs. This content is broken up in the following sections: Analysis and Coding Principles Assembling Source Data Tidy Data for Telemetry Preparing Input Data for crawl Determining Your Model Parameters Exploring and Troubleshooting Model Results Predicting a Movement Track Simulating Tracks from the Posterior Visualization of Results 2.1 Analysis and Coding Principles As with anything in science and R, there are a number of right ways to approach a problem. The workflows and principles outline here aren’t the only way to use crawl. However, this content has been developed after years of working with researchers and troubleshooting common issues. For most users, following this guide will prove a successful endeavor. More advanced users or those with specific needs should feel free to refer to this as a starting point but then expand to meet their need. 2.1.1 Source Data are Read Only Source data should not be edited. In many cases, the financial and personal investment required to acquire these data is significant. In addition, the unique timing, location, and nature of the data cannot be replicated so every effort should be employed to preserve the integrity of source data as they were collected. All efforts should be made to also insure that source data are stored in plain text or well described open formats. This approach provides researchers confidence that data will be available and usable well into the future. 2.1.2 Script Everything In all likelihood, the format of any source data will not be condusive to established analytical procedures. For example, crawl cannot accept a raw data file downloaded from ArgosWeb or the Wildlife Computers Data Portal. Some amount of data carpentry is required. Keeping with the principles of reproducibility, all data assembly and carpentry should be scripted. Here, we rely on the R programming language for our scripts, but Python or other similar languages will also meet this need. 2.1.3 Document Along the Way Scripting the data assembly should be combined with an effort to properly document the process. Documentation is an important component of reproducibility and, if done as part of the scritping and development process, provides an easier workflow for publishing results and data to journals and repositories. The rmarkdown, bookdown, and knitr packages provide an excellent framework for combining your R scripts with documentation. This entire book is written with bookdown and knitr. 2.1.4 Embrace the Tidyverse The tidyverse describes a set of R packages developed by, or in association with, Hadley Wickham. Tidy data principles are outlined in a 2014 paper published in the Journal of Statistical Software. The key tennants of tidy data are: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Location data from telemetry deployments often follows this type of structure: each location estimate is a row and the columns all represent variable attributes associated with that location. Additionally, the location data are usually organized into a single table. Behavior data, however, often comes in a less structured format. Different behavior type data are sometimes mixed within a single table and column headings do not consistently describe the same variable (e.g. Bin1, Bin2, etc). There are valid reasons for tag vendors and data providers to transmit the source data in this way, but the structure is not condusive to analysis. The tidyverse package is a wrapper for a number of separate packages that each contribute to the tidy’ing of data. The tidyr, dplyr, readr, and purrr packages are key components. In addition, the lubridate package (not included in the tidyverse package) is especially useful for consistent maninpulation of date-time values. 2.1.5 Anticipate Errors &amp; Trap Them Use R’s error trapping functions (try() and tryCatch()) in areas of your code where you anticipate parsing errors or potential issues with convergence or fit. The purrr::safely() function also provides a great service. 2.2 Assembling Source Data Where and how you access the source data for your telemetry study will depend on the type of tag and vendor. Argos location data is available to users from the Argos website and many third party sites and respositories (e.g. movebank, sea-turtle.org) have API connections to ArgosWeb that can faciliate data access. Each tag manufacturer often has additional data streams included within the Argos transmission that require specific software or processing to translate. Both the Sea Mammal Research Unit (SMRU) and Wildlife Computers provide online data portals that provide users access to the location and additional sensor data. Regardless of how the data are retrieved, these files should be treated as read only and not edited. ArgosWeb and vendors have, typically, kept the data formats and structure consistent over time. This affords end users the ability to develop custom processing scripts without much fear the formats will change and break their scripts. 2.2.1 Read In CSV Source Data Here we will demonstrate the development of such a script using data downloaded from the Wildlife Computers data portal. All data are grouped by deployment and presented in various comma-separated files. The *-Locations.csv contains all of the Argos location estimates determined for the deployment. At a minimum, this file includes identifying columns such as DeployID, PTT, Date, Quality, Type, Latitude, and Longitude. If the Argos Kalman filtering approach has been enabled (which it should be for any modern tag deployment), then additional data will be found in columns that describe the error ellipses (e.g. Error Semi-major axis, Error Semi-minor axis, Error Ellipse orientation). Note, if the tag transmitted GPS/FastLoc data, this file will list those records as Type = 'FastGPS'. The other file of interest we will be working with is an example *-Histos.csv. These files include dive and haul-out behavior data derived from sensors on the tag. Most notably, the pressure transducer (for depth) and the salt-water switch (for determining wet/dry status). We will focus on processing this file to demonstrate how we can properly tidy our data. We will rely on the tidyverse set of packages plus purrr and lubridate to make reading and processing these files easier and more reliable. library(tidyverse) library(purrr) library(lubridate) if(!require(devtools)) install.packages(&#39;devtools&#39;) library(sp) The readr includes the read_csv() function which we will rely on to read the csv data into R. path_to_file &lt;- &quot;examples/data/160941-Locations.csv&quot; tbl_locs &lt;- readr::read_csv(path_to_file) 2.2.2 Specify Column Data Types The readr::read_csv() function tries to interpret the proper data types for each of the columns and provides us the column specifications it determined. In most cases, the function gets this correct. However, if we examine the cols() specification selected, we see that the Date column was read in as a character data type. To correct this, we need to provide our own cols() specification. We can do this by simply modifying the specification readr::read_csv() provided us. In this case, we want the Date column to rely on col_datetime and to parse the character string using the format %H:%M:%S %d-%b-%Y. my_cols &lt;- cols( DeployID = col_character(), Ptt = col_integer(), Instr = col_character(), Date = col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), Type = col_character(), Quality = col_character(), Latitude = col_double(), Longitude = col_double(), `Error radius` = col_integer(), `Error Semi-major axis` = col_integer(), `Error Semi-minor axis` = col_integer(), `Error Ellipse orientation` = col_integer(), Offset = col_character(), `Offset orientation` = col_character(), `GPE MSD` = col_character(), `GPE U` = col_character(), Count = col_character(), Comment = col_character() ) tbl_locs &lt;- readr::read_csv(path_to_file,col_types = my_cols) 2.2.3 Import From Many Files In most instances you’ll likely want to read in data from multiple deployments. The purrr::map() function can be combined with readr::read_csv() and dplyr::bind_rows() make this easy. tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() tbl_locs %&gt;% group_by(DeployID) %&gt;% summarise(num_locs = n(), start_date = min(Date), end_date = max(Date)) ## # A tibble: 2 × 4 ## DeployID num_locs start_date end_date ## &lt;chr&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 PV2016_3019_16A0222 3064 2016-09-21 02:54:02 2017-02-15 08:15:22 ## 2 PV2016_3024_15A0909 976 2016-09-22 02:48:00 2017-02-15 05:06:20 2.3 Tidy Data for Telemetry A key tennat of tidy data is that each row represents a single observation. For location data, regardless of the data source, this is typical of the source data structure. Each line in a file usually represents a single location estimate at a specific time. So, there’s very little we need to do in order to get our tbl_locs into an acceptable structure. 2.3.1 Standard Column Names One thing we can do, is to adjust the column names so there are no spaces or hyphens. We can also drop everything to lower-case to avoid typos. To do this we will create our own function, make_names(), which expands on the base function make.names(). We also want to change the date column to a more appropriate and less confusing date_time. make_names &lt;- function(x) { new_names &lt;- make.names(colnames(x)) new_names &lt;- gsub(&quot;\\\\.&quot;, &quot;_&quot;, new_names) new_names &lt;- tolower(new_names) colnames(x) &lt;- new_names x } tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::rename(date_time = date) %&gt;% dplyr::arrange(deployid, date_time) At this point, our tbl_locs is pretty tidy and, other than a few final steps, ready for input into the crawl::crwMLE() function. The *-Histos.csv file, however, has a messy, horizontal structure and will require some tidying. The tidyr package has a number of options that will help us. But, first let’s look closer at the structure of a typical *-Histos.csv. 2.3.2 Tidy a Messy ‘Histos’ File Wildlife Computers provides a variety of data types within this file structure. Because the details of each data type are subject to tag programming details, the data structure places observations into a series of Bin1, Bin2, ..., Bin72 columns. What each Bin# column represents depends on the data type and how the tags were programmed. For instance, a Dive Depth record represents the number of dives with a maximum depth within the programmed depth bin range. Time At Depth (TAD) represents the number of seconds spent within a particular programmed depth bin range. Each record represents a period of time that is also user programmable. While this messy data structure is an efficient way of delivering the data to users, it violates the principles of tidy data strucutes and presents a number of data management and analysis challenges. We are going to focus on a process for tidying the Percent or 1Percent timeline data as this information can be incorporated into the crawl::crwMLE() model fit. The same approach should be adopted for other histogram record types and the wcUtils package provides a series of helper functions developed specifically for this challenge. The Percent/1Percent timeline data are presented within the *-Histos.csv file structure as 24-hour observations — each record represents a 24-hour UTC time period. Within the record, each Bin# represents an hour of the day from 00 through 23. The values recorded for each Bin# column represent the percentage of that hour the tag was dry (out of the water). This data structure violates the tidy principles of requiring each record represent a single observation and that each column represent a single variable. Let’s start by reading the data in to R with readr::read_csv() as before. Note, we again rely on a custom cols() specification to insure data types are properly set. And, since we are only interested in Bin columns 1:24, we will select out the remaining columns. my_cols &lt;- readr::cols( DeployID = readr::col_character(), Ptt = readr::col_character(), DepthSensor = readr::col_character(), Source = readr::col_character(), Instr = readr::col_character(), HistType = readr::col_character(), Date = readr::col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), `Time Offset` = readr::col_double(), Count = readr::col_integer(), BadTherm = readr::col_integer(), LocationQuality = readr::col_character(), Latitude = readr::col_double(), Longitude = readr::col_double(), NumBins = readr::col_integer(), Sum = readr::col_integer(), Bin1 = readr::col_double(), Bin2 = readr::col_double(), Bin3 = readr::col_double(), Bin4 = readr::col_double(), Bin5 = readr::col_double(), Bin6 = readr::col_double(), Bin7 = readr::col_double(), Bin8 = readr::col_double(), Bin9 = readr::col_double(), Bin10 = readr::col_double(), Bin11 = readr::col_double(), Bin12 = readr::col_double(), Bin13 = readr::col_double(), Bin14 = readr::col_double(), Bin15 = readr::col_double(), Bin16 = readr::col_double(), Bin17 = readr::col_double(), Bin18 = readr::col_double(), Bin19 = readr::col_double(), Bin20 = readr::col_double(), Bin21 = readr::col_double(), Bin22 = readr::col_double(), Bin23 = readr::col_double(), Bin24 = readr::col_double(), Bin25 = readr::col_double(), Bin26 = readr::col_double(), Bin27 = readr::col_double(), Bin28 = readr::col_double(), Bin29 = readr::col_double(), Bin30 = readr::col_double(), Bin31 = readr::col_double(), Bin32 = readr::col_double(), Bin33 = readr::col_double(), Bin34 = readr::col_double(), Bin35 = readr::col_double(), Bin36 = readr::col_double(), Bin37 = readr::col_double(), Bin38 = readr::col_double(), Bin39 = readr::col_double(), Bin40 = readr::col_double(), Bin41 = readr::col_double(), Bin42 = readr::col_double(), Bin43 = readr::col_double(), Bin44 = readr::col_double(), Bin45 = readr::col_double(), Bin46 = readr::col_double(), Bin47 = readr::col_double(), Bin48 = readr::col_double(), Bin49 = readr::col_double(), Bin50 = readr::col_double(), Bin51 = readr::col_double(), Bin52 = readr::col_double(), Bin53 = readr::col_double(), Bin54 = readr::col_double(), Bin55 = readr::col_double(), Bin56 = readr::col_double(), Bin57 = readr::col_double(), Bin58 = readr::col_double(), Bin59 = readr::col_double(), Bin60 = readr::col_double(), Bin61 = readr::col_double(), Bin62 = readr::col_double(), Bin63 = readr::col_double(), Bin64 = readr::col_double(), Bin65 = readr::col_double(), Bin66 = readr::col_double(), Bin67 = readr::col_double(), Bin68 = readr::col_double(), Bin69 = readr::col_double(), Bin70 = readr::col_double(), Bin71 = readr::col_double(), Bin72 = readr::col_double() ) tbl_percent &lt;- dir(&#39;examples/data/&#39;, &quot;*-Histos.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::select(-(bin25:bin72)) %&gt;% dplyr::arrange(deployid, date) 2.3.3 Gathering Data with tidyr Now that our percent timeline data are in R, we can work to tidy the data structure using the tidyr::gather() function. This function requires that we provide character names for the key and value columns to create. We then specify which columns will be gathered into those key-value paired columns. In this case, our new columns will be bin and percent_dry and we will gather the values from all of the columns that start with the character string bin. Since we know that each bin column reprents an hour of the day, we can also improve our data structure by changing the value of date_hour (note we rename date to date_hour) to reflect this. We accomplish this by creating a simple lookup table (tibble) that relates each bin column to the hour of day it represents. The dplyr::left_join() function is used to match and merge this with our tbl_percent and then we add the hour value to each date_hour value with the dplyr::mutate() and lubridate::hours() functions. ## Create a tbl_df that Relates Bin Columns to Day Hours bins &lt;- tibble(bin = paste(&quot;bin&quot;,1:24,sep = &quot;&quot;),hour = 0:23) ## Chain Together Multiple Commands to Create Our Tidy Dataset tbl_percent &lt;- tbl_percent %&gt;% tidyr::gather(bin,percent_dry, starts_with(&#39;bin&#39;)) %&gt;% dplyr::left_join(bins, by = &quot;bin&quot;) %&gt;% dplyr::rename(date_hour = date) %&gt;% dplyr::mutate(date_hour = date_hour + lubridate::hours(hour)) %&gt;% dplyr::select(deployid,date_hour,percent_dry) %&gt;% group_by(deployid, date_hour) %&gt;% summarize(percent_dry = mean(percent_dry)) %&gt;% ungroup() %&gt;% dplyr::arrange(deployid,date_hour) Examining tbl_percent, we see that the layout is now more vertical and more in line with tidy principles tbl_percent ## # A tibble: 5,564 × 3 ## deployid date_hour percent_dry ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 PV2016_3019_16A0222 2016-09-21 02:00:00 NA ## 2 PV2016_3019_16A0222 2016-09-21 03:00:00 NA ## 3 PV2016_3019_16A0222 2016-09-21 04:00:00 100 ## 4 PV2016_3019_16A0222 2016-09-21 05:00:00 100 ## 5 PV2016_3019_16A0222 2016-09-21 06:00:00 100 ## 6 PV2016_3019_16A0222 2016-09-21 07:00:00 30 ## 7 PV2016_3019_16A0222 2016-09-21 08:00:00 20 ## 8 PV2016_3019_16A0222 2016-09-21 09:00:00 10 ## 9 PV2016_3019_16A0222 2016-09-21 10:00:00 10 ## 10 PV2016_3019_16A0222 2016-09-21 11:00:00 10 ## # ... with 5,554 more rows 2.3.4 Visualizing Source Data To further explore our imported data and confirm there aren’t any remaining issues to address, we can create visual representations of the data. The first plot will rely on the leaflet package to create an interactive plot of the tbl_locs object. The second plot will create a heatmap of tbl_percent values. library(leaflet) if (!require(leaflet.extras)) devtools::install_github(&#39;bhaskarvk/leaflet.extras&#39;) library(leaflet.extras) library(ggthemes) library(sf) sf_locs &lt;- st_as_sf(tbl_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% st_set_crs(&quot;+init=epsg:4326&quot;) sf_lines &lt;- sf_locs %&gt;% st_geometry() %&gt;% st_cast(&quot;MULTIPOINT&quot;,ids = as.integer(as.factor(sf_locs$deployid))) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) %&gt;% st_sf(deployid = as.factor(unique(sf_locs$deployid))) pal &lt;- colorFactor(ggthemes::ptol_pal()(2), domain = sf_lines$deployid) m &lt;- leaflet(data = sf_locs) %&gt;% addProviderTiles(&quot;Esri.OceanBasemap&quot;) %&gt;% addPolylines(data = sf_lines, weight = 2, color = ~pal(deployid)) %&gt;% addCircleMarkers(radius = 2, weight = 2, opacity = 1, color = ~pal(deployid)) %&gt;% addLegend(pal = pal,values = ~deployid,labels = ~deployid) %&gt;% suspendScroll() m Figure 2.1: An interactive map of observed satellite telemetry locations from two harbor seals in the Aleutian Islands. library(viridis) p &lt;- tbl_percent %&gt;% dplyr::mutate(solar_hour = date_hour + lubridate::dhours(11.5), solar_hour = lubridate::hour(solar_hour)) %&gt;% ggplot(aes(x = lubridate::date(date_hour), y = solar_hour, fill = percent_dry)) + geom_tile() + scale_y_continuous(breaks = c(6,12,18)) + scale_fill_viridis(name = &quot;Percent Dry&quot;, guide = guide_colorbar( title.position = &quot;bottom&quot;, barwidth = 25, barheight = 1 )) + facet_wrap( ~ deployid, nrow = 2) + theme_fivethirtyeight() + theme(axis.title = element_text()) + ylab(&quot;local solar hour&quot;) + xlab(&quot;&quot;) + ggtitle(paste(&quot;Haul-out Behavior by Local Solar Hour&quot;)) + theme(panel.grid.major.x = element_blank()) p Figure 2.2: Depiction of hourly percent dry observations for two satellite tags deployed on harbor seals in the Aleutian Islands. 2.4 Preparing Input Data for crawl At this point, the tbl_locs object represents the minimal source data required to fit crawl::crwMLE(). In addition to including location estimates in the model fit, one can also include an activity parameter. For harbor seals, we can rely on the percent-timeline data as an indicator of activity — when the tags are dry for most of an hour, the seal is likely hauled out and not moving. The crawl::crwMLE() will interpret this as an indication that the auto-correlation parameter should go to zero. In this section, we will prepare two input objects for crawl::crwMLE(). The first will be a spatial object of location estimates that can be passed directly. The second, will be a data.frame that represents a merge between the location estimates and percent-timeline data. 2.4.1 Error Parameters for GPS Data Many tags deployed on marine animals now include the option to collect GPS quality location esimates in addition to Argos. crawl::crwMLE() relies on the ellipse error parameters provide by Argos, so we need to fill in the GPS locations with appropriate values. GPS locations from these tags are generally expected to have an error radius of ~50m as long as the number of satellites used for the estimate is greater than 4. Below, we will use the dplyr::mutate() function to fill in these values. While we are at it, we will also remove any Argos records that do not have associated error ellipse values. tbl_locs &lt;- tbl_locs %&gt;% dplyr::filter(!(is.na(error_radius) &amp; type == &#39;Argos&#39;)) %&gt;% dplyr::mutate( error_semi_major_axis = ifelse(type == &#39;FastGPS&#39;, 50, error_semi_major_axis), error_semi_minor_axis = ifelse(type == &#39;FastGPS&#39;, 50, error_semi_minor_axis), error_ellipse_orientation = ifelse(type == &#39;FastGPS&#39;, 0, error_ellipse_orientation) ) 2.4.2 Known Locations In many cases, researchers are also knowledgable of the time and location coordinates of animal release. The Wildlife Computers Data Portal allows this to be entered and is included as a User record. The error radius one might specify for this record is dependent on the release situation. In general, it is best to over estimate this error radius. This record should be the first location record in the dataset passed to crawl::crwMLE(). Some researchers may also have additional date-times during the deployment when the location is known (e.g. fur seals returning to a rookery between trips). In these cases, tbl_locs can be augmented with additional User locations, but the code example below will need to be customized for each situation. This code example presumes there is just one User location and that corresponds to the release/deployment start. tbl_locs &lt;- tbl_locs %&gt;% dplyr::mutate( error_semi_major_axis = ifelse(type == &#39;User&#39;, 500, error_semi_major_axis), error_semi_minor_axis = ifelse(type == &#39;User&#39;, 500, error_semi_minor_axis), error_ellipse_orientation = ifelse(type == &#39;User&#39;, 0, error_ellipse_orientation) ) user_dt &lt;- tbl_locs %&gt;% dplyr::rename(release_dt = date_time) %&gt;% dplyr::filter(type == &#39;User&#39;) %&gt;% select(deployid,release_dt) tbl_locs &lt;- tbl_locs %&gt;% left_join(user_dt, by = &#39;deployid&#39;) %&gt;% filter(date_time &gt;= release_dt) %&gt;% select(-release_dt) tbl_locs %&gt;% group_by(deployid) %&gt;% select(deployid,date_time,type) %&gt;% do(head(.,1)) ## Source: local data frame [2 x 3] ## Groups: deployid [2] ## ## deployid date_time type ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 PV2016_3019_16A0222 2016-09-21 05:08:00 User ## 2 PV2016_3024_15A0909 2016-09-22 02:48:00 User 2.4.3 Duplicate Times Argos data often contain near duplicate records. These are identified by location estimates with the same date-time but differing coordinate or error values. In theory, crawl::crwMLE() can handle these situations, but we have found it is more reliable to fix these records. The first option for fixing the records would be to eliminate one of the duplicate records. However, it is often not possible to reliably identify which record is more appropriate to discard. For this reason, we advocate adjusting the date-time value for one of the records and increasing the value by 1 second. To facilitate this, we will rely on the xts::make.time.unique() function. make_unique &lt;- function(x) { xts::make.time.unique(x$date_time,eps = 1) } library(xts) tbl_locs &lt;- tbl_locs %&gt;% dplyr::arrange(deployid,date_time) %&gt;% dplyr::group_by(deployid) %&gt;% tidyr::nest() %&gt;% dplyr::mutate(unique_time = purrr::map(data, make_unique)) %&gt;% tidyr::unnest() %&gt;% dplyr::select(-date_time) %&gt;% rename(date_time = unique_time) 2.4.4 Course Speed Filter This step is optional, but it is very common for Argos data to include obviously wrong locations (locations that are 100s of kilometers away from the study area). Including such obviously wrong locations in the analysis can result in unexpected issues or problems fitting. For this reason, we recommend a course speed filter to remove these obviously wrong locations. A typical speed filter might use a value of 2.5 m/s as a biologically reasonable value for pinnipeds. For this application, we will use 7.5 m/s and rely on the argosfilter package. The example code below follows a typical split-apply-combine approach using functions from the dplyr, tidyr, and purrr packages. To better understand what each step is doing, we suggest you step through the process and examine the results at point. speed_filt &lt;- function(x) { argosfilter::sdafilter( lat = x$latitude, lon = x$longitude, dtime = x$date_time, lc = x$quality, vmax = 7.5 ) } tbl_locs &lt;- tbl_locs %&gt;% dplyr::group_by(deployid) %&gt;% tidyr::nest() %&gt;% dplyr::mutate(filtered = purrr::map(data, speed_filt)) %&gt;% tidyr::unnest() %&gt;% dplyr::filter(filtered %in% c(&quot;not&quot;, &quot;end_location&quot;)) %&gt;% dplyr::select(-filtered) %&gt;% dplyr::arrange(deployid,date_time) 2.4.5 Create a Spatial Object There are now two package frameworks within R for describing spatial data: sp and sf. The sp package has been the defacto standard for spatial data in R and has widespread support in a number of additional packages. The sf package is new and still in active development. However, the sf package is based on the more open simple features standard for spatial data. For this reason, and others, many in the R community feel sf is the future of spatial data in R. The crawl::crwMLE() can accept either a SpatialPointsDataFrame from the sp package or an sf data.frame of POINT geometry types. Here, we will focus on the sf package. sf_locs &lt;- sf::st_as_sf(tbl_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% sf::st_set_crs(.,4326) Earlier versions of crawl allowed users to pass geographic coordinates (e.g. latitude, longitude). However, relying on geographic coordinates presents some problems (e.g. crossing 180 or 0) and requires some assumptions to convert decimal degrees into meters — the units for the provided error ellipses. Because of these issues, all data should be provided to the crawl::crwMLE() as projected coordinates in meter units. Which projection users choose is dependent upon their study area. For these data, we will go with the North Pole LAEA Bering Sea projection. This projection is often abbreciated by the EPSG code of 3571. The sf::transform() function provides an easy method for transforming our coordinates. After, we can examine the geometry and confirm our coordinate units are in meters. sf_locs &lt;- sf::st_transform(sf_locs, 3571) sf::st_geometry(sf_locs) ## Geometry set for 3787 features ## geometry type: POINT ## dimension: XY ## bbox: xmin: -510531.2 ymin: -4094081 xmax: -369854 ymax: -4034591 ## epsg (SRID): 3571 ## proj4string: +proj=laea +lat_0=90 +lon_0=180 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## First 5 geometries: At this point, sf_locs is a valid format for input to crawl::crwMLE(). However, if you want to include the percent-timeline data as an activity parameter, there’s bit more work to do. 2.4.6 Merging with Activity Data Since our activity data represent hourly records, we need to create a date_hour field within tbl_locs as the basis for our join. While we are at it, we’ll translate the simple feature geometry for our point coordinates into an x and y column (note the use of a local function sfc_as_cols() to facilitate this). sfc_as_cols &lt;- function(x, names = c(&quot;x&quot;,&quot;y&quot;)) { stopifnot(inherits(x,&quot;sf&quot;) &amp;&amp; inherits(sf::st_geometry(x),&quot;sfc_POINT&quot;)) ret &lt;- do.call(rbind,sf::st_geometry(x)) ret &lt;- tibble::as_tibble(ret) stopifnot(length(names) == ncol(ret)) ret &lt;- setNames(ret,names) dplyr::bind_cols(x,ret) } sf_locs &lt;- sf_locs %&gt;% mutate(date_hour = lubridate::floor_date(date_time,&#39;hour&#39;)) %&gt;% arrange(deployid, date_time) %&gt;% sfc_as_cols() We also need to make sure our activity data includes all possible hours during the deployment. There are often missing days where the percent-timeline data was not transmitted back and so are not included in the source data. We rely on the tidyr::complete() function to expand our date_hour field. This will results in NA values for percent_dry. We will set these values to 33 because it will allow some level of correlated movement during these missing periods. Another option would be to use the zoo::na_locf() function to carry the last ovserved value forward. tbl_percent &lt;- tbl_percent %&gt;% group_by(deployid) %&gt;% tidyr::complete(date_hour = full_seq(date_hour, 3600L)) %&gt;% dplyr::mutate(percent_dry = ifelse(is.na(percent_dry), 33, percent_dry)) In addition to joining these two tables together, we also need to make sure the activity data doesn’t extend beyond the range of date-time values for observed locations. trim_deployment &lt;- function(x) { dplyr::filter(x, between(date_hour, lubridate::floor_date(min(date_time,na.rm = TRUE),&#39;hour&#39;), max(date_time,na.rm = TRUE))) } tbl_locs_activity &lt;- tbl_percent %&gt;% dplyr::left_join(sf_locs, by = c(&quot;deployid&quot;,&quot;date_hour&quot;)) %&gt;% dplyr::mutate(activity = 1 - percent_dry/100) %&gt;% dplyr::group_by(deployid) %&gt;% tidyr::nest() %&gt;% dplyr::mutate(data = purrr::map(data,trim_deployment)) %&gt;% unnest() %&gt;% dplyr::mutate(date_time = ifelse(is.na(date_time),date_hour,date_time), date_time = lubridate::as_datetime(date_time)) %&gt;% dplyr::select(-geometry) %&gt;% dplyr::arrange(deployid,date_time) 2.5 Determining Your Model Parameters 2.5.1 Create a Nested Data Structure We now have two objects: sf_locs and tbl_locs_activity. The first step is to nest each of our objects by deployid so we can take advantage of list columns when fitting multiple deployments at once. We will group by deployid and then call tidyr::nest(). For sf_locs we need to convert back into an sf object after the group by and nesting. sf_locs &lt;- sf_locs %&gt;% dplyr::group_by(deployid) %&gt;% dplyr::arrange(date_time) %&gt;% tidyr::nest() %&gt;% dplyr::mutate(data = purrr::map(data,sf::st_as_sf)) tbl_locs_activity &lt;- tbl_locs_activity %&gt;% dplyr::group_by(deployid) %&gt;% dplyr::arrange(date_time) %&gt;% tidyr::nest() This nested data structure and the use of list-columns is a relatively new concept for R. This approach is all part of the tidyverse dialect and, instead of making this a full tutoriala on purrr::map(), tidyr::nest(), etc. I suggest reading up on various tutorials available on the web and playing around with other examples. 2.5.2 Create model.matrix for Ellipse Errors The data in this example are from recent Argos deployments and, thus, include error ellipse information determined from the Kalman filter processing that has been available since 2011. This ellipse information provides a significant improvement to modeling animal movement because it provides specific information on the error associated with each location estimate. The ellipse_matrix() function translates the error ellipse information provided by Argos into a model matrix structure that crawl::crwMLE() can utilize. The crawl::argosDiag2Cov() function further transforms this information back into the original data frame structure which we can then bind back into our data. ellipse_matrix &lt;- function(x) { if (inherits(x, &quot;sf&quot;)) { sf::st_geometry(x) &lt;- NULL } ret &lt;- model.matrix( ~ error_semi_major_axis + error_semi_minor_axis + error_ellipse_orientation, model.frame(~ .,x,na.action = na.pass))[,-1] ret } sf_locs &lt;- sf_locs %&gt;% dplyr::mutate(diag = purrr::map(data, ellipse_matrix), diag = purrr::map(diag, ~ crawl::argosDiag2Cov( .x[,1], .x[,2], .x[,3])), data = purrr::map2(data,diag,bind_cols) ) %&gt;% dplyr::select(-diag) tbl_locs_activity &lt;- tbl_locs_activity %&gt;% dplyr::mutate(diag = purrr::map(data, ellipse_matrix), diag = purrr::map(diag, ~ crawl::argosDiag2Cov( .x[,1], .x[,2], .x[,3])), data = purrr::map2(data,diag,bind_cols) ) %&gt;% dplyr::select(-diag) 2.5.3 Create Model Parameters We will create a function that will create our initial parameters (init) that crawl::crwMLE() requires. This is a list of starting values for the mean and variance-covariance for the initial state of the model. When choosing the initial parameters, it is typical to have the mean centered on the first observation with zero velocity. a is the starting location for the model – the first known coordinate; and P is a 4x4 var-cov matrix that specifies the error (in projected units) for the initial coordinates. init_params &lt;- function(d) { ret &lt;- list(a = c(d$x[1], 0, d$y[1], 0), P = diag(c(10 ^ 2, 10 ^ 2, 10 ^ 2, 10 ^ 2))) ret } In addition to specifying the initial state, we need to provide a vector specifying which parameters will be fixed. Any values specified as NA will be estimated by the model. This information is passed to crawl::crwMLE() as the functional parameter fixpar. For this example, the first two values in the vector are set to 1 as we are providing the error structure with the ellipse information. The thrid value in the vector is the sigma and this is almost always set to NA. The fourthe value is beta which is the autocorrelation parameter. This is also, typically, set to NA as we want to estimate this. However, in some cases, the movement data will be more representative of Brownian motion and the model will be more likely to fit successfully if this is fixed to a value of 4. If an activity parameter is provided, then a fifth value is inlcuded in fixpar and should be set to 0. insert section discussing fixpar values for Argos location quality classes It is also beneficial to provide some contraints for the model parameters you would like estimated. There are two typical scenarios: limiting the parameter range for various Argos location quality classes limiting the beta parameter estimation to be bounded by -4 and 4. In this example, we let the sigma parameter ranged from -Inf to Inf while limiting beta to range between -4 and 4. sf_locs &lt;- sf_locs %&gt;% dplyr::mutate(init = purrr::map(data,init_params), fixpar = rep( list(c(1,1,NA,NA)), nrow(.) ), constr = rep(list( list(lower = c(-Inf, -4), upper = (c(Inf, 4)))), nrow(.)) ) tbl_locs_activity &lt;- tbl_locs_activity %&gt;% dplyr::mutate(init = purrr::map(data,init_params), fixpar = rep( list(c(1,1,NA,NA,0)), nrow(.) ), constr = rep(list( list(lower = c(-Inf, -4), upper = (c(Inf, 4)))), nrow(.)) ) 2.6 Fitting with crawl::crwMLE() The crawl::crwMLE() function is the workhorse for fitting our observed data to a continuous-time correlated random walk model. At this point, we have our data structure setup and we’ve established our initial parameters, specified fixed values for any parameters we will not estimate, and provided some constraints on those parameter estimates. Instead of calling crawl::crwMLE() directly, we will create a wrapper function that will work better with our tidy approach. It also allows us to make sequencial calls to crawl::crwMLE() that adjust parameters in cases where the fit fails. There are two ways in which we can adjust the parameters: provide a prior (or, in this case, a series of priors) in the form of a function fix the beta parameter at 4 to specify Brownian motion A few other aspects about our wrapper function to note. We pass our observed data in as d and then the parameters init, fixpar, and constr. Additionally, we can specify tryBrownian = FALSE if we don’t want the model to fit with Brownian motion as the final try. Each of the prior functions is specified within the function and the function cycles through a for loop of calls to crawl::crwMLE() with each prior. Note, the first prior value is NULL to specify our first model fit try is without any prior. Our wrapper function also checks the observed data, d, for any activity column to determine whether activity parameter should be included in the model fit. fit_crawl &lt;- function(d, init, fixpar, constr, tryBrownian = TRUE) { priors &lt;- list( NULL, ln_prior = function(par) { dnorm(par[2], 4, 4, log = TRUE) }, lap_prior = function(par) { -abs(par[2] - 4) / 5 }, reg_prior = function(par) { dt(par[2] - 3, df = 1, log = TRUE) } ) #cycle through 4 different prior values. the first being no prior/NULL for (prior in priors) { fit &lt;- crawl::crwMLE( mov.model = ~ 1, if (any(colnames(d) == &quot;activity&quot;)) { activity &lt;- ~ I(activity) } else {activity &lt;- NULL}, err.model = list( x = ~ ln.sd.x - 1, y = ~ ln.sd.y - 1, rho = ~ error.corr ), data = d, Time.name = &quot;date_time&quot;, initial.state = init, fixPar = fixpar, prior = prior, constr = constr, attempts = 1, control = list( trace = 0 ), initialSANN = list( maxit = 1500, trace = 0 ) ) if (!any(is.nan(fit$se))) { return(fit) } } # at this point, the most likely reason for failing to fit is b/c the mov&#39;t is # more Brownian in nature. Here, we fix beta at 4 which specifies Brownian if (any(is.nan(fit$se)) &amp;&amp; tryBrownian) { fixPar = c(1, 1, NA, 4) fit &lt;- crawl::crwMLE( mov.model = ~ 1, err.model = list( x = ~ ln.sd.x - 1, y = ~ ln.sd.y - 1, rho = ~ error.corr ), data = d, Time.name = &quot;date_time&quot;, initial.state = init, fixPar = fixPar, attempts = 1, control = list( trace = 0 ), initialSANN = list( maxit = 500, trace = 0 ) ) } fit } The tidy and purrrific approach to model fitting is to store the model results and estimated parameters as a list-column within our nested tibble. This has the significant benefit of keeping all our data, model specifications, model fits, parameter estimates — and, next, our model predictions — nicely organized. this section relies on the crawl::tidy_crwFit() function which is only available with the latest devel version of crawl from GitHub. library(pander) tbl_locs_fit &lt;- sf_locs %&gt;% dplyr::mutate(fit = purrr::pmap(list(d = data,init = init, fixpar = fixpar,constr = constr), fit_crawl), params = map(fit, crawl::tidy_crwFit)) After successfully fitting our models, we can explore the parameter estimates that are a result of that fit. The first set of tables are for our two seal deployments and the model has been fit without any activity data. panderOptions(&#39;knitr.auto.asis&#39;, FALSE) tbl_locs_fit$params %&gt;% walk(pander::pander,caption = &quot;crwMLE fit parameters&quot;) crwMLE fit parameters term estimate std.error conf.low conf.high ln tau.x ln.sd.x 1 NA NA NA ln tau.y ln.sd.y 1 NA NA NA ln sigma (Intercept) 3.213 0.012 3.189 3.236 ln beta (Intercept) 4 1.544 0.973 7.027 logLik -46293 NA NA NA AIC 92589 NA NA NA crwMLE fit parameters term estimate std.error conf.low conf.high ln tau.x ln.sd.x 1 NA NA NA ln tau.y ln.sd.y 1 NA NA NA ln sigma (Intercept) 3.255 0.023 3.209 3.301 ln beta (Intercept) 4 NA NA NA logLik -15473 NA NA NA AIC 30947 NA NA NA tbl_locs_activity_fit &lt;- tbl_locs_activity %&gt;% dplyr::mutate(fit = purrr::pmap(list(d = data,init = init, fixpar = fixpar,constr = constr), fit_crawl), params = map(fit, crawl::tidy_crwFit)) Our second set of tables are for our two seal deployments and the model has been fit with inclusion of activity data. panderOptions(&#39;knitr.auto.asis&#39;, FALSE) tbl_locs_activity_fit$params %&gt;% walk(pander::pander,caption = &quot;crwMLE fit parameters&quot;) crwMLE fit parameters term estimate std.error conf.low conf.high ln tau.x ln.sd.x 1 NA NA NA ln tau.y ln.sd.y 1 NA NA NA ln sigma (Intercept) 3.377 0.012 3.353 3.401 ln beta (Intercept) 4 1.645 0.776 7.224 ln phi 0 NA NA NA logLik -63939 NA NA NA AIC 127882 NA NA NA crwMLE fit parameters term estimate std.error conf.low conf.high ln tau.x ln.sd.x 1 NA NA NA ln tau.y ln.sd.y 1 NA NA NA ln sigma (Intercept) 3.26 0.023 3.214 3.305 ln beta (Intercept) 4 NA NA NA logLik -15388 NA NA NA AIC 30778 NA NA NA 2.7 Exploring and Troubleshooting Model Results 2.8 Predicting a Movement Track At this point, we have a fitted model for each seal and the next obvious step is to visualize and explore the resulting fitted track. To accomplish this, we’ll rely on the crawl::crwPredict() function. This function requires two key arguments: an object.crwFit and a predTime argument. The first is simply the fit object from our model fit. The second can be one of two types: a vector of POSIXct or numeric values that provide additional prediction times beyond the times for our observed locations in the original data. This vector should not start before the first observation or end after the last observation. if the original data were provided as a POSIXct type (which, in this example, they were), then crawl::crwPredict() can derive a sequence of regularly spaced prediction times from the original data. The user only needs to provide a character string that describes the regularly spaced duration. This character string should correspond to the by argument of the seq.POSIXt function (e.g. ‘1 hour’, ‘30 mins’). tbl_locs_activity_fit &lt;- tbl_locs_activity_fit %&gt;% dplyr::mutate(predict = purrr::map(fit, crawl::crwPredict, predTime = &#39;1 hour&#39;)) tbl_locs_activity_fit$predict %&gt;% purrr::walk(crawl::crwPredictPlot) We still need to perform some additional processing of the prediction object before we can use the information in analysis or visualize the track. We will create a custom function as.sf() which will convert our crwPredict object into an sf object of either “POINT” or “MULTILINESTRING” as.sf &lt;- function(p,id,epsg,type,loctype) { p &lt;- sf::st_as_sf(p, coords = c(&quot;mu.x&quot;,&quot;mu.y&quot;)) %&gt;% dplyr::mutate(deployid = id) %&gt;% filter(loctype %in% loctype) %&gt;% sf::st_set_crs(.,epsg) if (type == &quot;POINT&quot;) return(p) if (type == &quot;LINE&quot;) { p &lt;- p %&gt;% dplyr::arrange(TimeNum) %&gt;% sf::st_geometry() %&gt;% st_cast(&quot;MULTIPOINT&quot;,ids = as.integer(as.factor(p$deployid))) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) %&gt;% st_sf(deployid = unique(p$deployid)) return(p) } } tbl_locs_activity_fit &lt;- tbl_locs_activity_fit %&gt;% dplyr::mutate(sf_points = purrr::map2(predict, deployid, as.sf, epsg = 3571, type = &quot;POINT&quot;, loctype = &quot;p&quot;), sf_line = purrr::map2(predict, deployid, as.sf, epsg = 3571, type = &quot;LINE&quot;, loctype = &quot;p&quot;)) sf_pred_lines &lt;- tbl_locs_activity_fit$sf_line %&gt;% lift(rbind)() %&gt;% sf::st_set_crs(3571) n &lt;- length(unique(sf_pred_lines$deployid)) pal &lt;- colorFactor(ggthemes::ptol_pal()(n), domain = sf_pred_lines$deployid) m &lt;- sf_pred_lines %&gt;% sf::st_transform(4326) %&gt;% leaflet() %&gt;% addProviderTiles(&quot;Esri.OceanBasemap&quot;) %&gt;% addPolylines(weight = 2, color = ~pal(deployid)) %&gt;% addLegend(pal = pal,values = ~deployid,labels = ~deployid) %&gt;% suspendScroll() m Figure 2.3: An interactive map of predicted tracks from two harbor seals in the Aleutian Islands. 2.9 Adjust Path Around Land The crawl package now includes a fix_path() function which will adjust any predicted path around land features using a least-cost function. bering_base &lt;- nPacMaps::bering(fortify = FALSE, simplify = FALSE) ## Data are polygon data ## Rgshhs: clipping 12 of 2118 polygons ... bering_base@polygons &lt;- lapply(bering_base@polygons, maptools::checkPolygonsHoles) poly_expand &lt;- 50000 res &lt;- 200 tmpland &lt;- raster::rasterTmpFile() r &lt;- raster::raster( ext = raster::extend(raster::extent(matrix(st_bbox(sf_pred_lines), 2)), poly_expand), resolution = res, crs = sp::CRS(&quot;+init=epsg:3571&quot;) ) land &lt;- raster::rasterize(bering_base, r, getCover = TRUE, background = 0, filename = tmpland) land &lt;- land / 100 land[land &lt; 1] &lt;- 0 water &lt;- raster::asFactor(1 - land) trans &lt;- gdistance::transition(water, &quot;areas&quot;, directions = 16)[[1]] .fix_path &lt;- function(prd, res_raster, trans) { m &lt;- crawl::fix_path(prd, res_raster = land, trans = trans) prd$mu.x &lt;- m[,&quot;mu.x&quot;] prd$mu.y &lt;- m[,&quot;mu.y&quot;] prd } tbl_locs_activity_fit &lt;- tbl_locs_activity_fit %&gt;% dplyr::mutate(sf_points_fix = purrr::map(predict, .fix_path, res_raster = land, trans = trans)) tbl_locs_activity_fit &lt;- tbl_locs_activity_fit %&gt;% dplyr::mutate(sf_lines_fix = purrr::map2(sf_points_fix,deployid, as.sf, epsg = 3571, type = &quot;LINE&quot;, loctype = &quot;p&quot;)) sf_pred_lines_fix &lt;- tbl_locs_activity_fit$sf_lines_fix %&gt;% lift(rbind)() %&gt;% sf::st_set_crs(3571) n &lt;- length(unique(sf_pred_lines_fix$deployid)) pal &lt;- colorFactor(ggthemes::ptol_pal()(n), domain = sf_pred_lines_fix$deployid) m &lt;- sf_pred_lines_fix %&gt;% sf::st_transform(4326) %&gt;% leaflet() %&gt;% addProviderTiles(&quot;Esri.OceanBasemap&quot;) %&gt;% addPolylines(weight = 2, color = ~pal(deployid)) %&gt;% addLegend(pal = pal,values = ~deployid,labels = ~deployid) %&gt;% suspendScroll() m Figure 2.4: An interactive map of predicted tracks from two harbor seals in the Aleutian Islands. The paths have been fixed to re-route around land features. 2.10 Simulating Tracks from the Posterior 2.11 Visualization of Results "]
]
