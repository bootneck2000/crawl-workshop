[
["crawl-practical.html", "3 A Pragmatic Guide for Analysis with crawl 3.1 Analysis and Coding Principles 3.2 Assembling Source Data 3.3 Tidy Data for Telemetry 3.4 Preparing Input Data for crawl 3.5 Determining Your Model Parameters 3.6 Exploring and Troubleshooting Model Results 3.7 Predicting a Movement Track 3.8 Simulating Tracks from the Posterior 3.9 Visualization of Results", " 3 A Pragmatic Guide for Analysis with crawl The crawl package is designed and built with the idea that it should be accessible and useful to a research biologist with some intermediate R skills and an understanding of the basic statistical theory behind the analysis of animal movement. This portion of the book will focus on providing the user with suggested principles, workflows, and pragmatic approaches that, if followed, should make analysis with crawl more efficient and reliable. As with anything in science and R, there are a number of right ways to approach a problem. The workflows and principles outline here aren’t the only way to use crawl. However, this content has been developed after years of working with researchers and troubleshooting common issues. For most users, following this guide will prove a successful endeavor. More advanced users or those with specific needs should feel free to refer to this as a starting point but then expand to meet their need. This content is broken up in the following sections: Analysis and Coding Principles Assembling Source Data Tidy Data for Telemetry Preparing Input Data for crawl Determining Your Model Parameters Exploring and Troubleshooting Model Results Predicting a Movement Track Simulating Tracks from the Posterior Visualization of Results 3.1 Analysis and Coding Principles 3.1.1 Source Data are Read Only 3.1.2 Script Everything 3.1.3 Document Along the Way 3.1.4 Embrace the Tidyverse 3.1.5 Anticipate Errors &amp; Trap Them 3.2 Assembling Source Data Where and how you access the source data for your telemetry study will depend on the type of tag and vendor. Argos location data is available to users from the Argos website and many third party sites and respositories (e.g. movebank, sea-turtle.org) have API connections to ArgosWeb that can faciliate data access. Each tag manufacturer often has additional data streams included within the Argos transmission that require specific software or processing to translate. Both the Sea Mammal Research Unit (SMRU) and Wildlife Computers provide online data portals that provide users access to the location and additional sensor data. Regardless of how the data are retrieved, these files should be treated as read only and not edited. ArgosWeb and vendors have, typically, kept the data formats and structure consistent over time. This affords end users the ability to develop custom processing scripts without much fear the formats will change and break their scripts. Here we will demonstrate the development of such a script using data downloaded from the Wildlife Computers data portal. All data are grouped by deployment and presented in various comma-separated files. The *-Locations.csv contains all of the Argos location estimates determined for the deployment. At a minimum, this file includes identifying columns such as DeployID, PTT, Date, Quality, Type, Latitude, and Longitude. If the Argos Kalman filtering approach has been enabled (which it should be for any modern tag deployment), then additional data will be found in columns that describe the error ellipses (e.g. Error Semi-major axis, Error Semi-minor axis, Error Ellipse orientation). Note, if the tag transmitted GPS/FastLoc data, this file will list those records as Type = 'FastGPS'. The other file of interest we will be working with is an example *-Histos.csv. These files include dive and haul-out behavior data derived from sensors on the tag. Most notably, the pressure transducer (for depth) and the salt-water switch (for determining wet/dry status). We will focus on processing this file to demonstrate how we can properly tidy our data. We will rely on the tidyverse set of packages plus purrr and lubridate to make reading and processing these files easier and more reliable. library(tidyverse) library(purrr) library(lubridate) The readr includes the read_csv() function which we will rely on to read the csv data into R. path_to_file &lt;- &quot;examples/data/160941-Locations.csv&quot; tbl_locs &lt;- readr::read_csv(path_to_file) ## Parsed with column specification: ## cols( ## DeployID = col_character(), ## Ptt = col_integer(), ## Instr = col_character(), ## Date = col_character(), ## Type = col_character(), ## Quality = col_character(), ## Latitude = col_double(), ## Longitude = col_double(), ## `Error radius` = col_integer(), ## `Error Semi-major axis` = col_integer(), ## `Error Semi-minor axis` = col_integer(), ## `Error Ellipse orientation` = col_integer(), ## Offset = col_character(), ## `Offset orientation` = col_character(), ## `GPE MSD` = col_character(), ## `GPE U` = col_character(), ## Count = col_character(), ## Comment = col_character() ## ) The readr::read_csv() function tries to interpret the proper data types for each of the columns and provides us the column specifications it determined. In most cases, the function gets this correct. However, if we examine the cols() specification selected, we see that the Date column was read in as a character data type. To correct this, we need to provide our own cols() specification. We can do this by simply modifying the specification readr::read_csv() provided us. In this case, we want the Date column to rely on col_datetime and to parse the character string using the format %H:%M:%S %d-%b-%Y. my_cols &lt;- cols( DeployID = col_character(), Ptt = col_integer(), Instr = col_character(), Date = col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), # changed from col_character() Type = col_character(), Quality = col_character(), Latitude = col_double(), Longitude = col_double(), `Error radius` = col_integer(), `Error Semi-major axis` = col_integer(), `Error Semi-minor axis` = col_integer(), `Error Ellipse orientation` = col_integer(), Offset = col_character(), `Offset orientation` = col_character(), `GPE MSD` = col_character(), `GPE U` = col_character(), Count = col_character(), Comment = col_character() ) tbl_locs &lt;- readr::read_csv(path_to_file,col_types = my_cols) In most instances you’ll likely want to read in data from multiple deployments. The purrr::map() function can be combined with readr::read_csv() and dplyr::bind_rows() make this easy. tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() 3.3 Tidy Data for Telemetry A key tennat of tidy data is that each row represents a single observation. For location data, regardless of the data source, this is typical of the source data structure. Each line in a file usually represents a single location estimate at a specific time. So, there’s very little we need to do in order to get our tbl_locs into an acceptable structure. One thing we can do, is to adjust the column names so there are no spaces or hyphens. We can also drop everything to lower-case to avoid typos. To do this we will create our own function, make_names(), which expands on the base function make.names(). We also want to change the date column to a more appropriate and less confusing date_time. make_names &lt;- function(x) { new_names &lt;- make.names(colnames(x)) new_names &lt;- gsub(&quot;\\\\.&quot;, &quot;_&quot;, new_names) new_names &lt;- tolower(new_names) colnames(x) &lt;- new_names x } tbl_locs &lt;- dir(&#39;examples/data/&#39;, &quot;*-Locations.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::rename(date_time = date) %&gt;% dplyr::arrange(deployid, date_time) At this point, our tbl_locs is pretty tidy and, other than a few final steps, ready for input into the crawl::crwMLE() function. The *-Histos.csv file, however, has a messy, horizontal structure and will require some tidying. The tidyr package has a number of options that will help us. But, first let’s look closer at the structure of a typical *-Histos.csv. Wildlife Computers provides a variety of data types within this file structure. Because the details of each data type are subject to tag programming details, the data structure places observations into a series of Bin1, Bin2, ..., Bin72 columns. What each Bin# column represents depends on the data type and how the tags were programmed. For instance, a Dive Depth record represents the number of dives with a maximum depth within the programmed depth bin range. Time At Depth (TAD) represents the number of seconds spent within a particular programmed depth bin range. Each record represents a period of time that is also user programmable. While this messy data structure is an efficient way of delivering the data to users, it violates the principles of tidy data strucutes and presents a number of data management and analysis challenges. We are going to focus on a process for tidying the Percent or 1Percent timeline data as this information can be incorporated into the crawl::crwMLE() model fit. The same approach should be adopted for other histogram record types and the wcUtils package provides a series of helper functions developed specifically for this challenge. The Percent/1Percent timeline data are presented within the *-Histos.csv file structure as 24-hour observations — each record represents a 24-hour UTC time period. Within the record, each Bin# represents an hour of the day from 00 through 23. The values recorded for each Bin# column represent the percentage of that hour the tag was dry (out of the water). This data structure violates the tidy principles of requiring each record represent a single observation and that each column represent a single variable. Let’s start by reading the data in to R with readr::read_csv() as before. Note, we again rely on a custom cols() specification to insure data types are properly set. And, since we are only interested in Bin columns 1:24, we will select out the remaining columns. my_cols &lt;- readr::cols( DeployID = readr::col_character(), Ptt = readr::col_character(), DepthSensor = readr::col_character(), Source = readr::col_character(), Instr = readr::col_character(), HistType = readr::col_character(), Date = readr::col_datetime(&quot;%H:%M:%S %d-%b-%Y&quot;), `Time Offset` = readr::col_double(), Count = readr::col_integer(), BadTherm = readr::col_integer(), LocationQuality = readr::col_character(), Latitude = readr::col_double(), Longitude = readr::col_double(), NumBins = readr::col_integer(), Sum = readr::col_integer(), Bin1 = readr::col_double(), Bin2 = readr::col_double(), Bin3 = readr::col_double(), Bin4 = readr::col_double(), Bin5 = readr::col_double(), Bin6 = readr::col_double(), Bin7 = readr::col_double(), Bin8 = readr::col_double(), Bin9 = readr::col_double(), Bin10 = readr::col_double(), Bin11 = readr::col_double(), Bin12 = readr::col_double(), Bin13 = readr::col_double(), Bin14 = readr::col_double(), Bin15 = readr::col_double(), Bin16 = readr::col_double(), Bin17 = readr::col_double(), Bin18 = readr::col_double(), Bin19 = readr::col_double(), Bin20 = readr::col_double(), Bin21 = readr::col_double(), Bin22 = readr::col_double(), Bin23 = readr::col_double(), Bin24 = readr::col_double(), Bin25 = readr::col_double(), Bin26 = readr::col_double(), Bin27 = readr::col_double(), Bin28 = readr::col_double(), Bin29 = readr::col_double(), Bin30 = readr::col_double(), Bin31 = readr::col_double(), Bin32 = readr::col_double(), Bin33 = readr::col_double(), Bin34 = readr::col_double(), Bin35 = readr::col_double(), Bin36 = readr::col_double(), Bin37 = readr::col_double(), Bin38 = readr::col_double(), Bin39 = readr::col_double(), Bin40 = readr::col_double(), Bin41 = readr::col_double(), Bin42 = readr::col_double(), Bin43 = readr::col_double(), Bin44 = readr::col_double(), Bin45 = readr::col_double(), Bin46 = readr::col_double(), Bin47 = readr::col_double(), Bin48 = readr::col_double(), Bin49 = readr::col_double(), Bin50 = readr::col_double(), Bin51 = readr::col_double(), Bin52 = readr::col_double(), Bin53 = readr::col_double(), Bin54 = readr::col_double(), Bin55 = readr::col_double(), Bin56 = readr::col_double(), Bin57 = readr::col_double(), Bin58 = readr::col_double(), Bin59 = readr::col_double(), Bin60 = readr::col_double(), Bin61 = readr::col_double(), Bin62 = readr::col_double(), Bin63 = readr::col_double(), Bin64 = readr::col_double(), Bin65 = readr::col_double(), Bin66 = readr::col_double(), Bin67 = readr::col_double(), Bin68 = readr::col_double(), Bin69 = readr::col_double(), Bin70 = readr::col_double(), Bin71 = readr::col_double(), Bin72 = readr::col_double() ) tbl_percent &lt;- dir(&#39;examples/data/&#39;, &quot;*-Histos.csv&quot;, full.names = TRUE) %&gt;% purrr::map(read_csv,col_types = my_cols) %&gt;% dplyr::bind_rows() %&gt;% make_names() %&gt;% dplyr::select(-(bin25:bin72)) %&gt;% dplyr::arrange(deployid, date) Now that our percent timeline data are in R, we can work to tidy the data structure using the tidyr::gather() function. This function requires that we provide character names for the key and value columns to create. We then specify which columns will be gathered into those key-value paired columns. In this case, our new columns will be bin and percent_dry and we will gather the values from all of the columns that start with the character string bin. Since we know that each bin column reprents an hour of the day, we can also improve our data structure by changing the value of date_hour (note we rename date to date_hour) to reflect this. We accomplish this by creating a simple lookup table (tibble) that relates each bin column to the hour of day it represents. The dplyr::left_join() function is used to match and merge this with our tbl_percent and then we add the hour value to each date_hour value with the dplyr::mutate() and lubridate::hours() functions. ## Create a tbl_df that Relates Bin Columns to Day Hours bins &lt;- tibble(bin=paste(&quot;bin&quot;,1:24,sep=&quot;&quot;),hour=0:23) ## Chain Together Multiple Commands to Create Our Tidy Dataset tbl_percent &lt;- tbl_percent %&gt;% tidyr::gather(bin,percent_dry, starts_with(&#39;bin&#39;)) %&gt;% dplyr::left_join(bins, by=&quot;bin&quot;) %&gt;% dplyr::rename(date_hour = date) %&gt;% dplyr::mutate(date_hour = date_hour + lubridate::hours(hour)) %&gt;% dplyr::select(deployid,date_hour,percent_dry) %&gt;% dplyr::arrange(deployid,date_hour) Examining tbl_percent, we see that the layout is now more vertical and more in line with tidy principles tbl_percent ## # A tibble: 2,520 × 3 ## deployid date_hour percent_dry ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 PV2016_3024_15A0909 2016-09-22 02:00:00 NA ## 2 PV2016_3024_15A0909 2016-09-22 03:00:00 NA ## 3 PV2016_3024_15A0909 2016-09-22 04:00:00 40 ## 4 PV2016_3024_15A0909 2016-09-22 05:00:00 10 ## 5 PV2016_3024_15A0909 2016-09-22 06:00:00 10 ## 6 PV2016_3024_15A0909 2016-09-22 07:00:00 40 ## 7 PV2016_3024_15A0909 2016-09-22 08:00:00 98 ## 8 PV2016_3024_15A0909 2016-09-22 09:00:00 100 ## 9 PV2016_3024_15A0909 2016-09-22 10:00:00 100 ## 10 PV2016_3024_15A0909 2016-09-22 11:00:00 100 ## # ... with 2,510 more rows To further explore our imported data and confirm there aren’t any remaining issues to address, we can create visual representations of the data. The first plot will rely on the leaflet package to create an interactive plot of the tbl_locs object. The second plot will create a heatmap of tbl_percent values. library(leaflet) library(sf) ## Linking to GEOS 3.6.1, GDAL 2.1.3, proj.4 4.9.3 sf_locs &lt;- st_as_sf(tbl_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% st_set_crs(&quot;+init=epsg:4326&quot;) %&gt;% dplyr::mutate(id = ifelse(deployid == unique(deployid)[1], 1, 2)) sf_lines &lt;- sf_locs %&gt;% st_geometry() %&gt;% st_cast(&quot;MULTIPOINT&quot;,ids = sf_locs$id) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) %&gt;% st_sf(deployid = as.factor(unique(sf_locs$deployid))) pal &lt;- colorFactor(ggthemes::ptol_pal()(2), domain = sf_lines$deployid) m &lt;- leaflet(data = sf_locs) %&gt;% addProviderTiles(&quot;Esri.OceanBasemap&quot;) %&gt;% addPolylines(data = sf_lines, weight = 2, color = ~pal(deployid)) %&gt;% addCircleMarkers(radius = 2, weight = 2, opacity = 1, color = ~pal(deployid)) %&gt;% addLegend(pal = pal,values = ~deployid) %&gt;% addMiniMap(tiles = providers$Esri.OceanBasemap, toggleDisplay = T) m 3.4 Preparing Input Data for crawl 3.5 Determining Your Model Parameters 3.6 Exploring and Troubleshooting Model Results 3.7 Predicting a Movement Track 3.8 Simulating Tracks from the Posterior 3.9 Visualization of Results "]
]
